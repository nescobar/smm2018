{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: TwitterAPI in /Users/nescobar/anaconda/envs/py36/lib/python3.6/site-packages\n",
      "Requirement already satisfied: requests in /Users/nescobar/anaconda/envs/py36/lib/python3.6/site-packages (from TwitterAPI)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/nescobar/anaconda/envs/py36/lib/python3.6/site-packages (from TwitterAPI)\n",
      "Requirement already satisfied: oauthlib>=0.6.2 in /Users/nescobar/anaconda/envs/py36/lib/python3.6/site-packages (from requests-oauthlib->TwitterAPI)\n"
     ]
    }
   ],
   "source": [
    "#Install packages\n",
    "#!pip install python-twitter\n",
    "!pip install TwitterAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, accuracy_score\n",
    "from TwitterAPI import TwitterAPI\n",
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from bs4 import BeautifulSoup\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file = \"/Users/nescobar/Dropbox/Indiana/Social_Media_Mining/Project/smm2018/data/raw/tweets_harvey_20180218.csv\"\n",
    "train_file = \"/Users/nescobar/Dropbox/Indiana/Social_Media_Mining/Project/smm2018/data/raw/csv/labelled_edited.csv\"\n",
    "\n",
    "consumer_key = 'FP94BfUftKeygulmcLcVqRNvX'\n",
    "consumer_secret = 'GfZtNuk6Zu6lTOYGWibXI95MjMyks6SSlBEykyLuYe4NEUgGUu'\n",
    "access_token_key = '961088185756393472-FopLzpw7n3CrHhbHoWv8BlnR1mZwhGH'\n",
    "access_token_secret= '62Trp0LVEPvAwoGPH4ov8D4TQe2eEaQETKIfXKCPZX6NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "api = TwitterAPI(consumer_key, consumer_secret, auth_type='oAuth2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessTweets:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._stopwords=set(stopwords.words('english')+list(punctuation)+['ATUSER','URL','IMG'])\n",
    "        \n",
    "    def get_data(query=\"\", source=\"file\", path=data_file, feed=\"search/tweets\",api=api, maxid=0, n=100):\n",
    "        try:\n",
    "            if source == \"file\":\n",
    "                harvey_df = pd.read_csv(train_file, encoding = 'ISO-8859-1')\n",
    "                return harvey_df\n",
    "            else:\n",
    "                if maxid == 0:\n",
    "                    return pd.read_json(json.dumps([t for t in api.request(feed, {'q':query,'count':n})]))\n",
    "                else:\n",
    "                    return pd.read_json(json.dumps([t for t in api.request(feed, {'q': query, 'count': n, 'max_id': maxid})]))            \n",
    "        except:\n",
    "            print(\"Error while getting data\")\n",
    "            return None\n",
    "    \n",
    "    def process_tweets(self, list_of_tweets):\n",
    "        processed_tweets=[]\n",
    "        for tweet in list_of_tweets:\n",
    "            processed_tweets.append((self._process_tweet(tweet)))\n",
    "        return processed_tweets\n",
    "    \n",
    "    def _process_tweet(self,tweet):\n",
    "        try:\n",
    "            # Unescape from HTML\n",
    "            #tweet = html.unescape(tweet)\n",
    "            tweet = BeautifulSoup(tweet, 'lxml').get_text()\n",
    "            # 3a. Convert to lower case\n",
    "            tweet = tweet.lower()\n",
    "            # 3b. Replace links with the word URL \n",
    "            tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',tweet) \n",
    "            # 3c. Replace @username with \"AT_USER\"\n",
    "            tweet = re.sub('@[^\\s]+',' ',tweet)                \n",
    "            # 3d. Replace #word with word \n",
    "            tweet = re.sub(r'#([^\\s]+)',r'\\1',tweet)\n",
    "            # 3e. Replace images with the word IMG \n",
    "            tweet = re.sub(r'\\bpic.twitter.com\\s+', ' ', tweet)\n",
    "            # 3f Keep only words with letters\n",
    "            tweet = re.sub('[^a-zA-Z]',' ',tweet)\n",
    "            # 3g. Remove RT\n",
    "            tweet = re.sub(r'\\brt([\\b\\s])', ' ', tweet)\n",
    "            # Apply Lemmatization\n",
    "            lemmatizer = WordNetLemmatizer()           \n",
    "            tweet = [lemmatizer.lemmatize(word) for word in tweet.split() if word not in self._stopwords and len(word)>1]  \n",
    "            return (\" \".join(tweet)).strip()\n",
    "        except:\n",
    "            print(\"Error with tweet: \", tweet)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process tweets from file\n",
    "tweet_processor = ProcessTweets()\n",
    "raw_tweets = tweet_processor.get_data()\n",
    "#[print(t) for t in raw_tweets['text'][:10]]\n",
    "cleaned_tweets = tweet_processor.process_tweets(raw_tweets['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 397 entries, 0 to 417\n",
      "Data columns (total 2 columns):\n",
      "text      397 non-null object\n",
      "target    397 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 9.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Create train DF \n",
    "train_df = pd.DataFrame(cleaned_tweets, columns=['text'])\n",
    "train_df['target'] = raw_tweets.label\n",
    "train_df=train_df.drop_duplicates('text')\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(397, 1253)\n",
      "(397, 1253)\n"
     ]
    }
   ],
   "source": [
    "# Build a dictionary of feature indices\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(train_df.text)\n",
    "print(X_train_vec.shape)\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_vec)\n",
    "print(X_train_tfidf.shape)\n",
    "\n",
    "#vectorizer.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, train_df.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Relocation', 'Relocation', 'Volunteering', 'Relocation',\n",
       "       'Volunteering', 'Money', 'Donation', 'Relocation'],\n",
       "      dtype='<U14')"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the classifier\n",
    "test_tweets = ['find animal', 'My dog is missing please help me find him','we need more volunteers to help', 'Shelters','medical nurse','money','blood','rebuild house']\n",
    "test_tweets=tweet_processor.process_tweets(test_tweets)\n",
    "\n",
    "X_test_vec = vectorizer.transform(test_tweets)\n",
    "X_test_tfidf = tfidf.transform(X_test_vec)\n",
    "\n",
    "predicted = clf.predict(X_test_tfidf)\n",
    "\n",
    "predicted\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
