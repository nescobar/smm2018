{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install packages\n",
    "#!pip install python-twitter\n",
    "#!pip install TwitterAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from TwitterAPI import TwitterAPI\n",
    "from nltk import PorterStemmer, WordNetLemmatizer, LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = \"/Users/nescobar/Dropbox/Indiana/Social_Media_Mining/Project/smm2018/data/raw/csv/annotation/training_set_final_4.csv\"\n",
    "test_file = \"/Users/nescobar/Dropbox/Indiana/Social_Media_Mining/Project/smm2018/data/raw/csv/annotation/random_new_t.csv\"\n",
    "\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token_key = ''\n",
    "access_token_secret= ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#api = TwitterAPI(consumer_key, consumer_secret, auth_type='oAuth2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessTweets:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.normalization = 'pt_stem'\n",
    "        self._stopwords=set(stopwords.words('english')+list(punctuation)+['AT_USER','URL','IMG']\n",
    "                                                                        +['twitter','com','twitter.com','pic','hurricaneharvey'])\n",
    "                                                                         #'texas','houston'])\n",
    "        \n",
    "    def get_data(query=\"\", source=\"file\", path=train_file, feed=\"search/tweets\",api=None, maxid=0, n=100):\n",
    "        try:\n",
    "            if source == \"file\":\n",
    "                harvey_df = pd.read_csv(path, encoding = 'ISO-8859-1')\n",
    "                return harvey_df\n",
    "            #else:\n",
    "            #    if maxid == 0:\n",
    "            #        return pd.read_json(json.dumps([t for t in api.request(feed, {'q':query,'count':n})]))\n",
    "            #    else:\n",
    "            #        return pd.read_json(json.dumps([t for t in api.request(feed, {'q': query, 'count': n, 'max_id': maxid})]))            \n",
    "        except:\n",
    "            print(\"Error while getting data\")\n",
    "            return None\n",
    "    \n",
    "    def process_tweets(self, list_of_tweets):\n",
    "        processed_tweets=[]\n",
    "        for tweet in list_of_tweets:\n",
    "            processed_tweets.append((self._process_tweet(tweet)))\n",
    "        return processed_tweets\n",
    "    \n",
    "    def _process_tweet(self,tweet):\n",
    "        try:\n",
    "            # Unescape from HTML\n",
    "            #tweet = html.unescape(tweet)\n",
    "            #tweet = BeautifulSoup(tweet, 'lxml').get_text()\n",
    "            # 3a. Convert to lower case\n",
    "            tweet = tweet.lower()\n",
    "            # 3b. Replace links with the word URL \n",
    "            tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',tweet) \n",
    "            # 3c. Replace @username with \"AT_USER\"\n",
    "            tweet = re.sub('@[^\\s]+',' ',tweet)                \n",
    "            # 3d. Replace #word with word \n",
    "            tweet = re.sub(r'#([^\\s]+)',r'\\1',tweet)\n",
    "            # 3e. Replace images with the word IMG \n",
    "            tweet = re.sub(r'\\bpic.twitter.com\\s+', ' ', tweet)\n",
    "            # 3f Keep only words with letters\n",
    "            tweet = re.sub('[^a-zA-Z]',' ',tweet)\n",
    "            # 3g. Remove RT\n",
    "            tweet = re.sub(r'\\brt([\\b\\s])', ' ', tweet)\n",
    "            \n",
    "            # Apply Lemmatization\n",
    "            if (self.normalization == 'wn_lem'):\n",
    "                lemmatizer = WordNetLemmatizer() \n",
    "                tweet = [lemmatizer.lemmatize(word) for word in tweet.split() if word not in self._stopwords and len(word)>1]  \n",
    "            elif (self.normalization == 'lc_stem'):\n",
    "                st = LancasterStemmer()\n",
    "                tweet = [st.stem(word) for word in tweet.split() if word not in self._stopwords and len(word)>1]  \n",
    "            elif (self.normalization == 'pt_stem'):\n",
    "                pt = PorterStemmer()\n",
    "                tweet = [pt.stem(word) for word in tweet.split() if word not in self._stopwords and len(word)>1]  \n",
    "            else:\n",
    "                lemmatizer = WordNetLemmatizer() \n",
    "                tweet = [lemmatizer.lemmatize(word) for word in tweet.split() if word not in self._stopwords and len(word)>1]  \n",
    "            \n",
    "            return (\" \".join(tweet)).strip()\n",
    "        \n",
    "        except:\n",
    "            print(\"Error with tweet: \", tweet)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process tweets from file\n",
    "#tweet_processor = ProcessTweets()\n",
    "#raw_tweets = tweet_processor.get_data()\n",
    "#train_df = pkl.load(open(\"training_set.pkl\", \"rb\"))\n",
    "#raw_tweets.drop('Unnamed: 0', 1, inplace=True)\n",
    "#raw_tweets.columns=['text','label']\n",
    "#[print(t) for t in raw_tweets['text'][:10]]\n",
    "#train_df['text'] = tweet_processor.process_tweets(raw_tweets['text'])\n",
    "#train_df.columns = ['text','target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train DF \n",
    "from sklearn.utils import shuffle\n",
    "import pickle as pkl\n",
    "\n",
    "#train_df = pd.DataFrame(cleaned_tweets, columns=['text'])\n",
    "#train_df = pkl.load(open(\"training_set.pkl\", \"rb\"))\n",
    "#train_df['target'] = raw_tweets.label\n",
    "#train_df = shuffle(train_df)\n",
    "#train_df=train_df.drop_duplicates('text')\n",
    "#train_df.info()\n",
    "#train_df\n",
    "tweet_processor = ProcessTweets()\n",
    "train_df = pkl.load(open(\"training_set.pkl\", \"rb\"))\n",
    "train_df['text'] = tweet_processor.process_tweets(raw_tweets['text'])\n",
    "train_df.columns = ['text','target']\n",
    "train_df = shuffle(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes:  0.878431372549\n",
      "Logistic Regression:  0.917647058824\n",
      "Logistic Regression with SGD:  0.933333333333\n",
      "SVM:  0.870588235294\n",
      "SVM with SGD:  0.929411764706\n",
      "Total running time 0.4950888156890869\n"
     ]
    }
   ],
   "source": [
    "# Testing with different classifiers\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from time import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "# Start running time\n",
    "t0 = time()\n",
    "\n",
    "# Build a pipeline that contains vectorizer, transform and classifier\n",
    "\n",
    "# Multinomial NB\n",
    "clf = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\")),\n",
    "                 ('clf', MultinomialNB()),\n",
    "                ])\n",
    "scores = cross_val_score(clf, train_df.text, train_df.target, cv=5)\n",
    "print(\"Multinomial Naive Bayes: \", np.mean(scores))\n",
    "\n",
    "# Logistic Regression\n",
    "clf_lr = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\")),\n",
    "                 ('clf', LogisticRegression()),\n",
    "                ])\n",
    "scores = cross_val_score(clf_lr, train_df.text, train_df.target, cv=5)\n",
    "print(\"Logistic Regression: \", np.mean(scores))\n",
    "\n",
    "# Logistic Regression with SGD \n",
    "clf_lr_sgd = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\")),\n",
    "                 ('clf', SGDClassifier(loss='log', penalty='l1',\n",
    "                                            alpha=1e-3, random_state=42,\n",
    "                                            max_iter=100, tol=None)),\n",
    "                ])\n",
    "scores = cross_val_score(clf_lr_sgd, train_df.text, train_df.target, cv=5)\n",
    "print(\"Logistic Regression with SGD: \", np.mean(scores))\n",
    "\n",
    "# SVM\n",
    "clf = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\")),\n",
    "                 ('clf', SVC()),\n",
    "                ])\n",
    "scores = cross_val_score(clf, train_df.text, train_df.target, cv=5)\n",
    "print(\"SVM: \", np.mean(scores))\n",
    "\n",
    "# SVM with SGD (Stochastic Gradient Descent) \n",
    "clf = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\")),\n",
    "                 ('clf', SGDClassifier(loss='hinge', penalty='l1',\n",
    "                                            alpha=1e-3, random_state=42,\n",
    "                                            max_iter=100, tol=None)),\n",
    "                ])\n",
    "scores = cross_val_score(clf, train_df.text, train_df.target, cv=5)\n",
    "print(\"SVM with SGD: \", np.mean(scores))\n",
    "\n",
    "# Print running time for training and predicting\n",
    "print('Total running time', time() - t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.933333333333\n",
      "{'clf__alpha': 0.001, 'clf__class_weight': None, 'clf__fit_intercept': True, 'clf__max_iter': 50, 'clf__penalty': 'l1', 'clf__warm_start': True, 'tfidf__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "# Parameter tuning with Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'tfidf__ngram_range' : [(1,1),(1,2)],\n",
    "             'clf__alpha': (1e-1, 1e-3, 1e-5),\n",
    "             'clf__max_iter': (10,50,100),\n",
    "             'clf__penalty': ('l1','l2','elasticnet'),\n",
    "             'clf__fit_intercept': (True,False),\n",
    "             'clf__class_weight': (None,'balanced'),\n",
    "             'clf__warm_start': (True, False)\n",
    "             }\n",
    "\n",
    "gs_clf = GridSearchCV(clf_lr_sgd, parameters, n_jobs=-1)\n",
    "gs_clf.fit(train_df.text, train_df.target)\n",
    "\n",
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time 0.39755702018737793\n",
      "['donation' 'relocation' 'volunteering']\n",
      "Total tweets analyzed 960\n",
      "Total tweets classified as DONATION 340 35.41666666666667%\n",
      "Total tweets classified as RELOCATION 25 2.604166666666667%\n",
      "Total tweets classified as VOLUNTEERING 36 3.75%\n",
      "Total tweets classified as NOT CLASSIFIED 559 58.229166666666664%\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "# Start running time\n",
    "t0 = time()\n",
    "\n",
    "# Get test data from recent tweets\n",
    "harvey_recent_df = pkl.load(open(\"harvey_august_2017.pkl\", \"rb\"))\n",
    "#harvey_tweets = tweet_processor.get_data(path=test_file)\n",
    "#harvey_tweets.columns=['text']\n",
    "# Drop duplicates\n",
    "harvey_recent_df.drop_duplicates('text', inplace=True)\n",
    "\n",
    "# Clean up testing data set\n",
    "clean_test_df = tweet_processor.process_tweets(harvey_recent_df.text)\n",
    "#harvey_tweets.drop_duplicates('text', inplace=True)\n",
    "#clean_test_df = tweet_processor.process_tweets(harvey_tweets['text'])\n",
    "#clean_test_df\n",
    "# Test the classifier\n",
    "#gs_clf.fit(train_df.text, train_df.target)\n",
    "predicted = gs_clf.predict_proba(clean_test_df)\n",
    "\n",
    "# Print running time for training and predicting\n",
    "print('Total running time', time() - t0)\n",
    "\n",
    "#print(metrics.classification_report(y_test, predicted))\n",
    "print(gs_clf.classes_)\n",
    "\n",
    "#print(metrics.classification_report(y_test, predicted))\n",
    "# Print predicted category for each tweet\n",
    "c_donation=c_other=c_relocation=c_volunteering=c_na=0\n",
    "for tweet, category in zip(harvey_recent_df.text, predicted):\n",
    "    #print('%r => %s' % (tweet, category))\n",
    "    # Consider only predictions with > 0.5 probability\n",
    "    if category[0] > 0.5:\n",
    "        c_donation += 1\n",
    "        #print('%r => %s' % (tweet, category))\n",
    "    #elif category[1] > 0.5:\n",
    "    #    c_other += 1\n",
    "    elif category[1] > 0.5:\n",
    "        c_relocation += 1\n",
    "        \n",
    "    elif category[2] > 0.5:\n",
    "        c_volunteering += 1\n",
    "    else:\n",
    "        c_na += 1\n",
    "        \n",
    "\n",
    "total_tweets = len(harvey_recent_df.text)\n",
    "print(\"Total tweets analyzed\", total_tweets)\n",
    "print(\"Total tweets classified as DONATION {} {}%\"\n",
    "      .format(c_donation, (c_donation/total_tweets)*100))\n",
    "#print(\"Total tweets classified as OTHER {} {}%\"\n",
    "#      .format(c_other, (c_other/total_tweets)*100))\n",
    "print(\"Total tweets classified as RELOCATION {} {}%\"\n",
    "      .format(c_relocation, (c_relocation/total_tweets)*100))\n",
    "print(\"Total tweets classified as VOLUNTEERING {} {}%\"\n",
    "      .format(c_volunteering, (c_volunteering/total_tweets)*100))\n",
    "print(\"Total tweets classified as NOT CLASSIFIED {} {}%\"\n",
    "      .format(c_na, (c_na/total_tweets)*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.57718519,  0.11725008,  0.30556473],\n",
       "       [ 0.36818036,  0.31979225,  0.31202739],\n",
       "       [ 0.76682582,  0.03907531,  0.19409887],\n",
       "       ..., \n",
       "       [ 0.35040743,  0.32606952,  0.32352305],\n",
       "       [ 0.76594021,  0.05720387,  0.17685592],\n",
       "       [ 0.40584469,  0.30224107,  0.29191424]])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "donation        85\n",
       "relocation      85\n",
       "volunteering    85\n",
       "dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['target']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SGDClassifier' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-4e1108b75924>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Create list of coefficients and words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcoefs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SGDClassifier' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "coefs = [] \n",
    "words = []\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words=\"english\")\n",
    "tfidf_vectorizer.fit_transform(train_df.text)\n",
    "\n",
    "# Create list of coefficients and words\n",
    "for k,v in tfidf_vectorizer.vocabulary_.items():\n",
    "    coefs.append(clf.named_steps['clf'].coef_[0][tfidf_vectorizer.vocabulary_[k]])\n",
    "    words.append(k)\n",
    "\n",
    "# Create pairs of (words, coefficients)\n",
    "pairs = []\n",
    "for p in zip(words, coefs): \n",
    "    pairs.append(p)\n",
    "\n",
    "# Sort pairs by coefficient\n",
    "pairs.sort(reverse=True, key=operator.itemgetter(1))\n",
    "\n",
    "# Print words with highest coefficients \n",
    "[print(k,v) for k,v in pairs[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('help', 136),\n",
       " ('donateaphoto', 131),\n",
       " ('houston', 122),\n",
       " ('hur', 114),\n",
       " ('texa', 104),\n",
       " ('harvey', 91),\n",
       " ('jnj', 87),\n",
       " ('jnj donateaphoto', 87),\n",
       " ('hurricanemar', 81),\n",
       " ('hurricaneirm', 79),\n",
       " ('christmas', 77),\n",
       " ('famy', 73),\n",
       " ('stil', 67),\n",
       " ('new', 64),\n",
       " ('don', 61),\n",
       " ('hom', 59),\n",
       " ('nee', 59),\n",
       " ('us', 57),\n",
       " ('via', 55),\n",
       " ('flood', 54)]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top ngrams in tweets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "#filter_file = \"/Users/nescobar/Dropbox/Indiana/Social_Media_Mining/Project/data/csv/filter_tweets_water.csv\"\n",
    "\n",
    "#filtered_tweets = tweet_processor.get_data(path=filter_file)\n",
    "#cleaned_filtered_tweets = tweet_processor.process_tweets(filtered_tweets['text'])\n",
    "\n",
    "vect = TfidfVectorizer(ngram_range=(1,2))\n",
    "summaries = \"\".join(clean_test_df)\n",
    "ngrams_summaries = vect.build_analyzer()(summaries)\n",
    "\n",
    "[bigram for bigram in Counter(ngrams_summaries).most_common(20) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
