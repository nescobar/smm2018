{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: TwitterAPI in /Users/nescobar/anaconda/envs/py36/lib/python3.6/site-packages\n",
      "Requirement already satisfied: requests in /Users/nescobar/anaconda/envs/py36/lib/python3.6/site-packages (from TwitterAPI)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/nescobar/anaconda/envs/py36/lib/python3.6/site-packages (from TwitterAPI)\n",
      "Requirement already satisfied: oauthlib>=0.6.2 in /Users/nescobar/anaconda/envs/py36/lib/python3.6/site-packages (from requests-oauthlib->TwitterAPI)\n"
     ]
    }
   ],
   "source": [
    "#Install packages\n",
    "#!pip install python-twitter\n",
    "!pip install TwitterAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from TwitterAPI import TwitterAPI\n",
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from bs4 import BeautifulSoup\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_file = \"/Users/nescobar/Dropbox/Indiana/Social_Media_Mining/Project/smm2018/data/raw/tweets_harvey_20180218.csv\"\n",
    "train_file = \"/Users/nescobar/Dropbox/Indiana/Social_Media_Mining/Project/smm2018/data/raw/csv/annotation/training_set_final_2.csv\"\n",
    "test_file = \"/Users/nescobar/Dropbox/Indiana/Social_Media_Mining/Project/smm2018/data/raw/tweets_harvey_recent.csv\"\n",
    "\n",
    "consumer_key = 'FP94BfUftKeygulmcLcVqRNvX'\n",
    "consumer_secret = 'GfZtNuk6Zu6lTOYGWibXI95MjMyks6SSlBEykyLuYe4NEUgGUu'\n",
    "access_token_key = '961088185756393472-FopLzpw7n3CrHhbHoWv8BlnR1mZwhGH'\n",
    "access_token_secret= '62Trp0LVEPvAwoGPH4ov8D4TQe2eEaQETKIfXKCPZX6NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = TwitterAPI(consumer_key, consumer_secret, auth_type='oAuth2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessTweets:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._stopwords=set(list(punctuation)+['ATUSER','URL','IMG'])\n",
    "        \n",
    "    def get_data(query=\"\", source=\"file\", path=train_file, feed=\"search/tweets\",api=api, maxid=0, n=100):\n",
    "        try:\n",
    "            if source == \"file\":\n",
    "                harvey_df = pd.read_csv(path, encoding = 'ISO-8859-1')\n",
    "                return harvey_df\n",
    "            else:\n",
    "                if maxid == 0:\n",
    "                    return pd.read_json(json.dumps([t for t in api.request(feed, {'q':query,'count':n})]))\n",
    "                else:\n",
    "                    return pd.read_json(json.dumps([t for t in api.request(feed, {'q': query, 'count': n, 'max_id': maxid})]))            \n",
    "        except:\n",
    "            print(\"Error while getting data\")\n",
    "            return None\n",
    "    \n",
    "    def process_tweets(self, list_of_tweets):\n",
    "        processed_tweets=[]\n",
    "        for tweet in list_of_tweets:\n",
    "            processed_tweets.append((self._process_tweet(tweet)))\n",
    "        return processed_tweets\n",
    "    \n",
    "    def _process_tweet(self,tweet):\n",
    "        try:\n",
    "            # Unescape from HTML\n",
    "            #tweet = html.unescape(tweet)\n",
    "            tweet = BeautifulSoup(tweet, 'lxml').get_text()\n",
    "            # 3a. Convert to lower case\n",
    "            tweet = tweet.lower()\n",
    "            # 3b. Replace links with the word URL \n",
    "            tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',tweet) \n",
    "            # 3c. Replace @username with \"AT_USER\"\n",
    "            tweet = re.sub('@[^\\s]+',' ',tweet)                \n",
    "            # 3d. Replace #word with word \n",
    "            tweet = re.sub(r'#([^\\s]+)',r'\\1',tweet)\n",
    "            # 3e. Replace images with the word IMG \n",
    "            tweet = re.sub(r'\\bpic.twitter.com\\s+', ' ', tweet)\n",
    "            # 3f Keep only words with letters\n",
    "            tweet = re.sub('[^a-zA-Z]',' ',tweet)\n",
    "            # 3g. Remove RT\n",
    "            tweet = re.sub(r'\\brt([\\b\\s])', ' ', tweet)\n",
    "            # Apply Lemmatization\n",
    "            lemmatizer = WordNetLemmatizer()           \n",
    "            tweet = [lemmatizer.lemmatize(word) for word in tweet.split() if word not in self._stopwords and len(word)>1]  \n",
    "            return (\" \".join(tweet)).strip()\n",
    "        except:\n",
    "            print(\"Error with tweet: \", tweet)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process tweets from file\n",
    "tweet_processor = ProcessTweets()\n",
    "raw_tweets = tweet_processor.get_data()\n",
    "#[print(t) for t in raw_tweets['text'][:10]]\n",
    "cleaned_tweets = tweet_processor.process_tweets(raw_tweets['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 333 entries, 0 to 349\n",
      "Data columns (total 2 columns):\n",
      "text      333 non-null object\n",
      "target    333 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 7.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Create train DF \n",
    "train_df = pd.DataFrame(cleaned_tweets, columns=['text'])\n",
    "train_df['target'] = raw_tweets.label\n",
    "train_df=train_df.drop_duplicates('text')\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('high water', 104),\n",
       " ('water rescue', 58),\n",
       " ('flood water', 53),\n",
       " ('water level', 43),\n",
       " ('rising water', 20),\n",
       " ('water crossing', 20),\n",
       " ('low water', 18),\n",
       " ('low water crossing', 17),\n",
       " ('high water rescue', 16),\n",
       " ('corpus christi', 15),\n",
       " ('water rise', 14),\n",
       " ('stay safe', 14),\n",
       " ('turn around', 13),\n",
       " ('food water', 13),\n",
       " ('port lavaca', 13),\n",
       " ('lot water', 12)]"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top ngrams in tweets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "filter_file = \"/Users/nescobar/Dropbox/Indiana/Social_Media_Mining/Project/data/csv/filter_tweets_water.csv\"\n",
    "\n",
    "filtered_tweets = tweet_processor.get_data(path=filter_file)\n",
    "cleaned_filtered_tweets = tweet_processor.process_tweets(filtered_tweets['text'])\n",
    "\n",
    "vect = TfidfVectorizer(ngram_range=(2,5))\n",
    "summaries = \"\".join(cleaned_filtered_tweets)\n",
    "ngrams_summaries = vect.build_analyzer()(summaries)\n",
    "\n",
    "[bigram for bigram in Counter(ngrams_summaries).most_common(20) if \"harvey\" not in bigram[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes:  0.582501928858\n",
      "Logistic Regression:  0.690384973528\n",
      "Logistic Regression with SGD:  0.732677521483\n",
      "SVM:  0.348335860803\n",
      "SVM with SGD:  0.738425519462\n",
      "Total running time 0.766855001449585\n"
     ]
    }
   ],
   "source": [
    "# Testing with different classifiers\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from time import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "# Start running time\n",
    "t0 = time()\n",
    "\n",
    "# Build a pipeline that contains vectorizer, transform and classifier\n",
    "\n",
    "# Multinomial NB\n",
    "clf = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\")),\n",
    "                 ('clf', MultinomialNB()),\n",
    "                ])\n",
    "scores = cross_val_score(clf, train_df.text, train_df.target, cv=5)\n",
    "print(\"Multinomial Naive Bayes: \", np.mean(scores))\n",
    "\n",
    "# Logistic Regression\n",
    "clf_lr = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\")),\n",
    "                 ('clf', LogisticRegression()),\n",
    "                ])\n",
    "scores = cross_val_score(clf_lr, train_df.text, train_df.target, cv=5)\n",
    "print(\"Logistic Regression: \", np.mean(scores))\n",
    "\n",
    "# Logistic Regression with SGD \n",
    "clf_lr_sgd = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\")),\n",
    "                 ('clf', SGDClassifier(loss='log', penalty='l1',\n",
    "                                            alpha=1e-3, random_state=42,\n",
    "                                            max_iter=100, tol=None)),\n",
    "                ])\n",
    "scores = cross_val_score(clf_lr_sgd, train_df.text, train_df.target, cv=5)\n",
    "print(\"Logistic Regression with SGD: \", np.mean(scores))\n",
    "\n",
    "# SVM\n",
    "clf = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\")),\n",
    "                 ('clf', SVC()),\n",
    "                ])\n",
    "scores = cross_val_score(clf, train_df.text, train_df.target, cv=5)\n",
    "print(\"SVM: \", np.mean(scores))\n",
    "\n",
    "# SVM with SGD (Stochastic Gradient Descent) \n",
    "clf = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\")),\n",
    "                 ('clf', SGDClassifier(loss='hinge', penalty='l1',\n",
    "                                            alpha=1e-3, random_state=42,\n",
    "                                            max_iter=100, tol=None)),\n",
    "                ])\n",
    "scores = cross_val_score(clf, train_df.text, train_df.target, cv=5)\n",
    "print(\"SVM with SGD: \", np.mean(scores))\n",
    "\n",
    "# Print running time for training and predicting\n",
    "print('Total running time', time() - t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.741741741742\n",
      "{'clf__alpha': 0.001, 'clf__class_weight': None, 'clf__fit_intercept': True, 'clf__max_iter': 100, 'clf__penalty': 'l1', 'clf__warm_start': True, 'tfidf__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "# Parameter tuning with Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'tfidf__ngram_range' : [(1,1),(1,2)],\n",
    "             'clf__alpha': (1e-1, 1e-3, 1e-5),\n",
    "             'clf__max_iter': (10,50,100),\n",
    "             'clf__penalty': ('l1','l2','elasticnet'),\n",
    "             'clf__fit_intercept': (True,False),\n",
    "             'clf__class_weight': (None,'balanced'),\n",
    "             'clf__warm_start': (True, False)\n",
    "             }\n",
    "\n",
    "gs_clf = GridSearchCV(clf_lr_sgd, parameters, n_jobs=-1)\n",
    "gs_clf.fit(train_df.text, train_df.target)\n",
    "\n",
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time 0.6519608497619629\n",
      "['donation' 'other' 'relocation' 'volunteering']\n",
      "Total tweets analyzed 960\n",
      "Total tweets classified as DONATION 166\n",
      "Total tweets classified as OTHER 58\n",
      "Total tweets classified as RELOCATION 16\n",
      "Total tweets classified as VOLUNTEERING 8\n",
      "Total tweets not classified 712\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "# Start running time\n",
    "t0 = time()\n",
    "\n",
    "# Get test data from recent tweets\n",
    "#test_df = tweet_processor.get_data(dtype=\"test\")\n",
    "harvey_recent_df = pkl.load(open(\"harvey_august_2017.pkl\", \"rb\"))\n",
    "# Drop duplicates\n",
    "harvey_recent_df.drop_duplicates('text', inplace=True)\n",
    "\n",
    "# Clean up testing data set\n",
    "clean_test_df = tweet_processor.process_tweets(harvey_recent_df.text)\n",
    "#clean_test_df\n",
    "# Test the classifier\n",
    "#gs_clf.fit(train_df.text, train_df.target)\n",
    "predicted = gs_clf.predict_proba(clean_test_df)\n",
    "\n",
    "# Print running time for training and predicting\n",
    "print('Total running time', time() - t0)\n",
    "\n",
    "#print(metrics.classification_report(y_test, predicted))\n",
    "print(gs_clf.classes_)\n",
    "\n",
    "#print(metrics.classification_report(y_test, predicted))\n",
    "# Print predicted category for each tweet\n",
    "c_donation=c_other=c_relocation=c_volunteering=c_na=0\n",
    "for tweet, category in zip(harvey_recent_df.text, predicted):\n",
    "    #print('%r => %s' % (tweet, category))\n",
    "    # Consider only predictions with > 0.5 probability\n",
    "    if category[0] > 0.5:\n",
    "        c_donation += 1\n",
    "    elif category[1] > 0.5:\n",
    "        c_other += 1\n",
    "    elif category[2] > 0.5:\n",
    "        c_relocation += 1\n",
    "    elif category[3] > 0.5:\n",
    "        c_volunteering += 1\n",
    "    else:\n",
    "        c_na += 1\n",
    "\n",
    "print(\"Total tweets analyzed\", len(harvey_recent_df.text))\n",
    "print(\"Total tweets classified as DONATION\", c_donation)\n",
    "print(\"Total tweets classified as OTHER\", c_other)\n",
    "print(\"Total tweets classified as RELOCATION\", c_relocation)\n",
    "print(\"Total tweets classified as VOLUNTEERING\", c_volunteering)\n",
    "print(\"Total tweets not classified\", c_na)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "donation        116\n",
       "other            75\n",
       "relocation       70\n",
       "volunteering     72\n",
       "dtype: int64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['target']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donation 12.0640678469\n",
      "blood 9.74489107167\n",
      "donate 5.10338825477\n",
      "money 3.6113738653\n",
      "consider 3.20580859584\n",
      "donating 3.1223009019\n",
      "send 2.76990962551\n",
      "raise 2.73999569517\n",
      "raising 2.2229595642\n",
      "supply 1.75875600847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "coefs = [] \n",
    "words = []\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words=\"english\")\n",
    "tfidf_vectorizer.fit_transform(train_df.text)\n",
    "\n",
    "# Create list of coefficients and words\n",
    "for k,v in tfidf_vectorizer.vocabulary_.items():\n",
    "    coefs.append(clf.named_steps['clf'].coef_[0][tfidf_vectorizer.vocabulary_[k]])\n",
    "    words.append(k)\n",
    "\n",
    "# Create pairs of (words, coefficients)\n",
    "pairs = []\n",
    "for p in zip(words, coefs): \n",
    "    pairs.append(p)\n",
    "\n",
    "# Sort pairs by coefficient\n",
    "pairs.sort(reverse=True, key=operator.itemgetter(1))\n",
    "\n",
    "# Print words with highest coefficients \n",
    "[print(k,v) for k,v in pairs[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
