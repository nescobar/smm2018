{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Install packages\n",
    "#!pip install python-twitter\n",
    "#!pip install TwitterAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from TwitterAPI import TwitterAPI\n",
    "from nltk import PorterStemmer, WordNetLemmatizer, LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_file = \"/Users/nescobar/Dropbox/Indiana/Social_Media_Mining/Project/smm2018/data/raw/csv/annotation/training_set_final_4.csv\"\n",
    "#test_file = \"/Users/nescobar/Dropbox/Indiana/Social_Media_Mining/Project/smm2018/data/raw/csv/annotation/random_new_t.csv\"\n",
    "\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token_key = ''\n",
    "access_token_secret= ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#api = TwitterAPI(consumer_key, consumer_secret, auth_type='oAuth2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ProcessTweets:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.normalization = 'pt_stem'\n",
    "        self._stopwords=set(stopwords.words('english')+list(punctuation)+['AT_USER','URL','IMG']\n",
    "                                                                        +['twitter','com','twitter.com','pic','hurricaneharvey'])\n",
    "                                                                         #'texas','houston'])\n",
    "        \n",
    "    def get_data(query=\"\", source=\"file\", path=None, feed=\"search/tweets\",api=None, maxid=0, n=100):\n",
    "        try:\n",
    "            if source == \"file\":\n",
    "                harvey_df = pd.read_csv(path, encoding = 'ISO-8859-1')\n",
    "                return harvey_df\n",
    "            #else:\n",
    "            #    if maxid == 0:\n",
    "            #        return pd.read_json(json.dumps([t for t in api.request(feed, {'q':query,'count':n})]))\n",
    "            #    else:\n",
    "            #        return pd.read_json(json.dumps([t for t in api.request(feed, {'q': query, 'count': n, 'max_id': maxid})]))            \n",
    "        except:\n",
    "            print(\"Error while getting data\")\n",
    "            return None\n",
    "    \n",
    "    def process_tweets(self, list_of_tweets):\n",
    "        processed_tweets=[]\n",
    "        for tweet in list_of_tweets:\n",
    "            processed_tweets.append((self._process_tweet(tweet)))\n",
    "        return processed_tweets\n",
    "    \n",
    "    def _process_tweet(self,tweet):\n",
    "        try:\n",
    "            # Unescape from HTML\n",
    "            #tweet = html.unescape(tweet)\n",
    "            #tweet = BeautifulSoup(tweet, 'lxml').get_text()\n",
    "            # 3a. Convert to lower case\n",
    "            tweet = tweet.lower()\n",
    "            # 3b. Replace links with the word URL \n",
    "            tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',tweet) \n",
    "            # 3c. Replace @username with \"AT_USER\"\n",
    "            tweet = re.sub('@[^\\s]+',' ',tweet)                \n",
    "            # 3d. Replace #word with word \n",
    "            tweet = re.sub(r'#([^\\s]+)',r'\\1',tweet)\n",
    "            # 3e. Replace images with the word IMG \n",
    "            tweet = re.sub(r'\\bpic.twitter.com\\s+', ' ', tweet)\n",
    "            # 3f Keep only words with letters\n",
    "            tweet = re.sub('[^a-zA-Z]',' ',tweet)\n",
    "            # 3g. Remove RT\n",
    "            tweet = re.sub(r'\\brt([\\b\\s])', ' ', tweet)\n",
    "            \n",
    "            # Apply Lemmatization\n",
    "            if (self.normalization == 'wn_lem'):\n",
    "                lemmatizer = WordNetLemmatizer() \n",
    "                tweet = [lemmatizer.lemmatize(word) for word in tweet.split() if word not in self._stopwords and len(word)>1]  \n",
    "            elif (self.normalization == 'lc_stem'):\n",
    "                st = LancasterStemmer()\n",
    "                tweet = [st.stem(word) for word in tweet.split() if word not in self._stopwords and len(word)>1]  \n",
    "            elif (self.normalization == 'pt_stem'):\n",
    "                pt = PorterStemmer()\n",
    "                tweet = [pt.stem(word) for word in tweet.split() if word not in self._stopwords and len(word)>1]  \n",
    "            else:\n",
    "                lemmatizer = WordNetLemmatizer() \n",
    "                tweet = [lemmatizer.lemmatize(word) for word in tweet.split() if word not in self._stopwords and len(word)>1]  \n",
    "            \n",
    "            return (\" \".join(tweet)).strip()\n",
    "        \n",
    "        except:\n",
    "            print(\"Error with tweet: \", tweet)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create train DF \n",
    "from sklearn.utils import shuffle\n",
    "import pickle as pkl\n",
    "\n",
    "tweet_processor = ProcessTweets()\n",
    "train_df = pkl.load(open(\"training_set.pkl\", \"rb\"))\n",
    "train_df['text'] = tweet_processor.process_tweets(train_df['text'])\n",
    "train_df.columns = ['text','target']\n",
    "train_df = shuffle(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes:  0.878431372549\n",
      "Logistic Regression:  0.933333333333\n",
      "Logistic Regression with SGD:  0.933333333333\n",
      "SVM:  0.870588235294\n",
      "SVM with SGD:  0.960784313725\n",
      "Total running time 0.4905397891998291\n"
     ]
    }
   ],
   "source": [
    "# Testing with different classifiers\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from time import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "# Start running time\n",
    "t0 = time()\n",
    "\n",
    "# Build a pipeline that contains vectorizer, transform and classifier\n",
    "\n",
    "# Multinomial NB\n",
    "clf = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\")),\n",
    "                 ('clf', MultinomialNB()),\n",
    "                ])\n",
    "scores = cross_val_score(clf, train_df.text, train_df.target, cv=5)\n",
    "print(\"Multinomial Naive Bayes: \", np.mean(scores))\n",
    "\n",
    "# Logistic Regression\n",
    "clf_lr = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\")),\n",
    "                 ('clf', LogisticRegression()),\n",
    "                ])\n",
    "scores = cross_val_score(clf_lr, train_df.text, train_df.target, cv=5)\n",
    "print(\"Logistic Regression: \", np.mean(scores))\n",
    "\n",
    "# Logistic Regression with SGD \n",
    "clf_lr_sgd = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\")),\n",
    "                 ('clf', SGDClassifier(loss='log', penalty='l1',\n",
    "                                            alpha=1e-3, random_state=42,\n",
    "                                            max_iter=100, tol=None)),\n",
    "                ])\n",
    "scores = cross_val_score(clf_lr_sgd, train_df.text, train_df.target, cv=5)\n",
    "print(\"Logistic Regression with SGD: \", np.mean(scores))\n",
    "\n",
    "# SVM\n",
    "clf = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\")),\n",
    "                 ('clf', SVC()),\n",
    "                ])\n",
    "scores = cross_val_score(clf, train_df.text, train_df.target, cv=5)\n",
    "print(\"SVM: \", np.mean(scores))\n",
    "\n",
    "# SVM with SGD (Stochastic Gradient Descent) \n",
    "clf = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\")),\n",
    "                 ('clf', SGDClassifier(loss='hinge', penalty='l1',\n",
    "                                            alpha=1e-3, random_state=42,\n",
    "                                            max_iter=100, tol=None)),\n",
    "                ])\n",
    "scores = cross_val_score(clf, train_df.text, train_df.target, cv=5)\n",
    "print(\"SVM with SGD: \", np.mean(scores))\n",
    "\n",
    "# Print running time for training and predicting\n",
    "print('Total running time', time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.972549019608\n",
      "{'clf__alpha': 1e-05, 'clf__class_weight': None, 'clf__fit_intercept': True, 'clf__max_iter': 50, 'clf__penalty': 'l1', 'clf__warm_start': True, 'tfidf__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "# Parameter tuning with Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'tfidf__ngram_range' : [(1,1),(1,2)],\n",
    "             'clf__alpha': (1e-1, 1e-3, 1e-5),\n",
    "             'clf__max_iter': (10,50,100),\n",
    "             'clf__penalty': ('l1','l2','elasticnet'),\n",
    "             'clf__fit_intercept': (True,False),\n",
    "             'clf__class_weight': (None,'balanced'),\n",
    "             'clf__warm_start': (True, False)\n",
    "             }\n",
    "\n",
    "gs_clf = GridSearchCV(clf_lr_sgd, parameters, n_jobs=-1)\n",
    "gs_clf.fit(train_df.text, train_df.target)\n",
    "\n",
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['donation' 'relocation' 'volunteering']\n",
      "File:  harvey_august_2017.pkl\n",
      "Total tweets analyzed: 984\n",
      "Total tweets classified as DONATION: 365 37.0%\n",
      "Total tweets classified as RELOCATION: 236 24.0%\n",
      "Total tweets classified as VOLUNTEERING: 199 20.0%\n",
      "Total tweets classified as NOT CLASSIFIED: 184 19.0%\n",
      "**************************************************\n",
      "File:  harvey_september_2017.pkl\n",
      "Total tweets analyzed: 987\n",
      "Total tweets classified as DONATION: 497 50.0%\n",
      "Total tweets classified as RELOCATION: 133 13.0%\n",
      "Total tweets classified as VOLUNTEERING: 182 18.0%\n",
      "Total tweets classified as NOT CLASSIFIED: 175 18.0%\n",
      "**************************************************\n",
      "File:  harvey_oct_dec_2017.pkl\n",
      "Total tweets analyzed: 949\n",
      "Total tweets classified as DONATION: 324 34.0%\n",
      "Total tweets classified as RELOCATION: 177 19.0%\n",
      "Total tweets classified as VOLUNTEERING: 191 20.0%\n",
      "Total tweets classified as NOT CLASSIFIED: 257 27.0%\n",
      "**************************************************\n",
      "File:  harvey_january_2018.pkl\n",
      "Total tweets analyzed: 985\n",
      "Total tweets classified as DONATION: 334 34.0%\n",
      "Total tweets classified as RELOCATION: 148 15.0%\n",
      "Total tweets classified as VOLUNTEERING: 224 23.0%\n",
      "Total tweets classified as NOT CLASSIFIED: 279 28.0%\n",
      "**************************************************\n",
      "File:  harvey_february_2018.pkl\n",
      "Total tweets analyzed: 989\n",
      "Total tweets classified as DONATION: 381 39.0%\n",
      "Total tweets classified as RELOCATION: 186 19.0%\n",
      "Total tweets classified as VOLUNTEERING: 231 23.0%\n",
      "Total tweets classified as NOT CLASSIFIED: 191 19.0%\n",
      "**************************************************\n",
      "File:  harvey_march_2018.pkl\n",
      "Total tweets analyzed: 984\n",
      "Total tweets classified as DONATION: 325 33.0%\n",
      "Total tweets classified as RELOCATION: 173 18.0%\n",
      "Total tweets classified as VOLUNTEERING: 283 29.0%\n",
      "Total tweets classified as NOT CLASSIFIED: 203 21.0%\n",
      "**************************************************\n",
      "File:  harvey_april_2018.pkl\n",
      "Total tweets analyzed: 935\n",
      "Total tweets classified as DONATION: 359 38.0%\n",
      "Total tweets classified as RELOCATION: 178 19.0%\n",
      "Total tweets classified as VOLUNTEERING: 228 24.0%\n",
      "Total tweets classified as NOT CLASSIFIED: 170 18.0%\n",
      "**************************************************\n",
      "Total running time 4.217618942260742\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "# Start running time\n",
    "t0 = time()\n",
    "\n",
    "pkl_files = ['harvey_august_2017.pkl', 'harvey_september_2017.pkl', 'harvey_oct_dec_2017.pkl'\n",
    "            ,'harvey_january_2018.pkl','harvey_february_2018.pkl', 'harvey_march_2018.pkl', \n",
    "             'harvey_april_2018.pkl']\n",
    "\n",
    "print(gs_clf.classes_)\n",
    "\n",
    "# Loop over each .pkl file\n",
    "for file in pkl_files:\n",
    "    harvey_recent_df = pkl.load(open(file, \"rb\"))\n",
    "    harvey_recent_df.drop_duplicates('text', inplace=True)\n",
    "    clean_test_df = tweet_processor.process_tweets(harvey_recent_df.text)\n",
    "    predicted = gs_clf.predict_proba(clean_test_df)\n",
    "    \n",
    "    c_donation=c_other=c_relocation=c_volunteering=c_na=0\n",
    "    for tweet, category in zip(harvey_recent_df.text, predicted):\n",
    "\n",
    "        # Consider only predictions with > 0.5 probability\n",
    "        if category[0] > 0.5:\n",
    "            c_donation += 1\n",
    "        elif category[1] > 0.5:\n",
    "            c_relocation += 1    \n",
    "        elif category[2] > 0.5:\n",
    "            c_volunteering += 1\n",
    "        else:\n",
    "            c_na += 1\n",
    "\n",
    "    total_tweets = len(harvey_recent_df.text)\n",
    "    print(\"File: \", file)\n",
    "    print(\"Total tweets analyzed:\", total_tweets)\n",
    "    print(\"Total tweets classified as DONATION: {} {}%\"\n",
    "            .format(c_donation, np.around((c_donation/total_tweets)*100),decimals=2))\n",
    "    print(\"Total tweets classified as RELOCATION: {} {}%\"\n",
    "            .format(c_relocation, np.around((c_relocation/total_tweets)*100),decimals=2))\n",
    "    print(\"Total tweets classified as VOLUNTEERING: {} {}%\"\n",
    "            .format(c_volunteering, np.around((c_volunteering/total_tweets)*100),decimals=2))\n",
    "    print(\"Total tweets classified as NOT CLASSIFIED: {} {}%\"\n",
    "            .format(c_na, np.around((c_na/total_tweets)*100),decimals=2))\n",
    "\n",
    "    print(\"*\"*50)\n",
    "    \n",
    "print('Total running time', time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5444483 ,  0.21327866,  0.24227304],\n",
       "       [ 0.42658016,  0.3072784 ,  0.26614144],\n",
       "       [ 0.45202719,  0.31336168,  0.23461113],\n",
       "       ..., \n",
       "       [ 0.40005084,  0.28816854,  0.31178063],\n",
       "       [ 0.82042923,  0.00932606,  0.17024471],\n",
       "       [ 0.42658016,  0.3072784 ,  0.26614144]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "donation        85\n",
       "relocation      85\n",
       "volunteering    85\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['target']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donat 2.98541714119\n",
      "blood 1.98904089138\n",
      "money 1.61216513005\n",
      "rais 1.18705777748\n",
      "consid 0.930953543509\n",
      "make 0.829254781668\n",
      "fundrais 0.770181604168\n",
      "brought 0.770181604168\n",
      "dc 0.770181604168\n",
      "autism 0.764404531788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "coefs = [] \n",
    "words = []\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words=\"english\")\n",
    "tfidf_vectorizer.fit_transform(train_df.text)\n",
    "\n",
    "clf_lr.fit(train_df.text, train_df.target)\n",
    "\n",
    "# Create list of coefficients and words\n",
    "for k,v in tfidf_vectorizer.vocabulary_.items():\n",
    "    coefs.append(clf_lr.named_steps['clf'].coef_[0][tfidf_vectorizer.vocabulary_[k]])\n",
    "    words.append(k)\n",
    "\n",
    "# Create pairs of (words, coefficients)\n",
    "pairs = []\n",
    "for p in zip(words, coefs): \n",
    "    pairs.append(p)\n",
    "\n",
    "# Sort pairs by coefficient\n",
    "pairs.sort(reverse=True, key=operator.itemgetter(1))\n",
    "\n",
    "# Print words with highest coefficients \n",
    "[print(k,v) for k,v in pairs[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Top ngrams in tweets\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from collections import Counter\n",
    "\n",
    "# vect = TfidfVectorizer(ngram_range=(1,2))\n",
    "# summaries = \"\".join(clean_test_df)\n",
    "# ngrams_summaries = vect.build_analyzer()(summaries)\n",
    "\n",
    "# [bigram for bigram in Counter(ngrams_summaries).most_common(20) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
