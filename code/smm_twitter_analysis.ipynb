{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: TwitterAPI in /Users/nescobar/anaconda/envs/py36/lib/python3.6/site-packages\n",
      "Requirement already satisfied: requests in /Users/nescobar/anaconda/envs/py36/lib/python3.6/site-packages (from TwitterAPI)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/nescobar/anaconda/envs/py36/lib/python3.6/site-packages (from TwitterAPI)\n",
      "Requirement already satisfied: oauthlib>=0.6.2 in /Users/nescobar/anaconda/envs/py36/lib/python3.6/site-packages (from requests-oauthlib->TwitterAPI)\n"
     ]
    }
   ],
   "source": [
    "#Install packages\n",
    "#!pip install python-twitter\n",
    "!pip install TwitterAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from TwitterAPI import TwitterAPI\n",
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from bs4 import BeautifulSoup\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_file = \"/Users/nescobar/Dropbox/Indiana/Social_Media_Mining/Project/smm2018/data/raw/tweets_harvey_20180218.csv\"\n",
    "train_file = \"/Users/nescobar/Dropbox/Indiana/Social_Media_Mining/Project/smm2018/data/raw/csv/labelled_edited.csv\"\n",
    "test_file = \"/Users/nescobar/Dropbox/Indiana/Social_Media_Mining/Project/smm2018/data/raw/tweets_harvey_recent.csv\"\n",
    "\n",
    "consumer_key = 'FP94BfUftKeygulmcLcVqRNvX'\n",
    "consumer_secret = 'GfZtNuk6Zu6lTOYGWibXI95MjMyks6SSlBEykyLuYe4NEUgGUu'\n",
    "access_token_key = '961088185756393472-FopLzpw7n3CrHhbHoWv8BlnR1mZwhGH'\n",
    "access_token_secret= '62Trp0LVEPvAwoGPH4ov8D4TQe2eEaQETKIfXKCPZX6NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "api = TwitterAPI(consumer_key, consumer_secret, auth_type='oAuth2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessTweets:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._stopwords=set(stopwords.words('english')+list(punctuation)+['ATUSER','URL','IMG'])\n",
    "        \n",
    "    def get_data(query=\"\", source=\"file\", path=train_file, feed=\"search/tweets\",api=api, maxid=0, n=100):\n",
    "        try:\n",
    "            if source == \"file\":\n",
    "                harvey_df = pd.read_csv(path, encoding = 'ISO-8859-1')\n",
    "                return harvey_df\n",
    "            else:\n",
    "                if maxid == 0:\n",
    "                    return pd.read_json(json.dumps([t for t in api.request(feed, {'q':query,'count':n})]))\n",
    "                else:\n",
    "                    return pd.read_json(json.dumps([t for t in api.request(feed, {'q': query, 'count': n, 'max_id': maxid})]))            \n",
    "        except:\n",
    "            print(\"Error while getting data\")\n",
    "            return None\n",
    "    \n",
    "    def process_tweets(self, list_of_tweets):\n",
    "        processed_tweets=[]\n",
    "        for tweet in list_of_tweets:\n",
    "            processed_tweets.append((self._process_tweet(tweet)))\n",
    "        return processed_tweets\n",
    "    \n",
    "    def _process_tweet(self,tweet):\n",
    "        try:\n",
    "            # Unescape from HTML\n",
    "            #tweet = html.unescape(tweet)\n",
    "            tweet = BeautifulSoup(tweet, 'lxml').get_text()\n",
    "            # 3a. Convert to lower case\n",
    "            tweet = tweet.lower()\n",
    "            # 3b. Replace links with the word URL \n",
    "            tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',tweet) \n",
    "            # 3c. Replace @username with \"AT_USER\"\n",
    "            tweet = re.sub('@[^\\s]+',' ',tweet)                \n",
    "            # 3d. Replace #word with word \n",
    "            tweet = re.sub(r'#([^\\s]+)',r'\\1',tweet)\n",
    "            # 3e. Replace images with the word IMG \n",
    "            tweet = re.sub(r'\\bpic.twitter.com\\s+', ' ', tweet)\n",
    "            # 3f Keep only words with letters\n",
    "            tweet = re.sub('[^a-zA-Z]',' ',tweet)\n",
    "            # 3g. Remove RT\n",
    "            tweet = re.sub(r'\\brt([\\b\\s])', ' ', tweet)\n",
    "            # Apply Lemmatization\n",
    "            lemmatizer = WordNetLemmatizer()           \n",
    "            tweet = [lemmatizer.lemmatize(word) for word in tweet.split() if word not in self._stopwords and len(word)>1]  \n",
    "            return (\" \".join(tweet)).strip()\n",
    "        except:\n",
    "            print(\"Error with tweet: \", tweet)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process tweets from file\n",
    "tweet_processor = ProcessTweets()\n",
    "raw_tweets = tweet_processor.get_data()\n",
    "#[print(t) for t in raw_tweets['text'][:10]]\n",
    "#cleaned_tweets = tweet_processor.process_tweets(raw_tweets['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 387 entries, 0 to 406\n",
      "Data columns (total 2 columns):\n",
      "text      387 non-null object\n",
      "target    387 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 9.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Create train DF \n",
    "train_df = pd.DataFrame(cleaned_tweets, columns=['text'])\n",
    "train_df['target'] = raw_tweets.label\n",
    "train_df=train_df.drop_duplicates('text')\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('public transportation', 12),\n",
       " ('transportation network', 9),\n",
       " ('need transportation', 8),\n",
       " ('transportation infrastructure', 7),\n",
       " ('transportation shelter', 5),\n",
       " ('state transportation', 5),\n",
       " ('transportation houston', 5),\n",
       " ('evacuee back', 5),\n",
       " ('people transportation', 4),\n",
       " ('without transportation', 4),\n",
       " ('help transportation', 4),\n",
       " ('transportation system', 4),\n",
       " ('dog houston', 4)]"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top ngrams in tweets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "filter_file = \"/Users/nescobar/Dropbox/Indiana/Social_Media_Mining/Project/data/csv/filter_tweets_transportation.csv\"\n",
    "\n",
    "filtered_tweets = tweet_processor.get_data(path=filter_file)\n",
    "cleaned_filtered_tweets = tweet_processor.process_tweets(filtered_tweets['text'])\n",
    "\n",
    "vect = TfidfVectorizer(ngram_range=(2,5))\n",
    "summaries = \"\".join(cleaned_filtered_tweets)\n",
    "ngrams_summaries = vect.build_analyzer()(summaries)\n",
    "\n",
    "[bigram for bigram in Counter(ngrams_summaries).most_common(20) if \"harvey\" not in bigram[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(193, 799)\n",
      "(193, 799)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, accuracy_score\n",
    "\n",
    "# Train/test split training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df.text, train_df.target, test_size=0.5, random_state=42)\n",
    "\n",
    "# Build a dictionary of feature indices\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "print(X_train_vec.shape)\n",
    "\n",
    "# Apply TF-IDF to the dictionary\n",
    "tfidf = TfidfTransformer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_vec)\n",
    "print(X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time 0.20992588996887207\n",
      "0.896907216495\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       Animals       1.00      1.00      1.00        22\n",
      "      Donation       0.71      1.00      0.83        50\n",
      "         Money       0.00      0.00      0.00        20\n",
      "    Relocation       1.00      1.00      1.00        47\n",
      "Transportation       1.00      1.00      1.00         9\n",
      "  Volunteering       1.00      1.00      1.00        46\n",
      "\n",
      "   avg / total       0.82      0.90      0.85       194\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nescobar/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Testing with Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from time import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "# Start running time\n",
    "t0 = time()\n",
    "\n",
    "# Build a pipeline that contains vectorizer, transform and classifier\n",
    "naive_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB()),\n",
    "                     ])\n",
    "\n",
    "naive_clf.fit(X_train, y_train)\n",
    "\n",
    "# Get test data from recent tweets\n",
    "#test_df = tweet_processor.get_data(dtype=\"test\")\n",
    "\n",
    "# Clean up testing data set\n",
    "clean_test_df = tweet_processor.process_tweets(y_test)\n",
    "\n",
    "# Test the classifier\n",
    "predicted = naive_clf.predict(clean_test_df)\n",
    "\n",
    "# Print running time for training and predicting\n",
    "print('Total running time', time() - t0)\n",
    "\n",
    "# Evaluate accuracy of the model\n",
    "accuracy = accuracy_score(y_test, predicted)\n",
    "print(accuracy)\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted))\n",
    "# Print predicted category for each tweet\n",
    "#for tweet, category in zip(test_tweets.text, predicted):\n",
    "#    print('%r => %s' % (tweet, category))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time 0.24737000465393066\n",
      "1.0\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       Animals       1.00      1.00      1.00        22\n",
      "      Donation       1.00      1.00      1.00        50\n",
      "         Money       1.00      1.00      1.00        20\n",
      "    Relocation       1.00      1.00      1.00        47\n",
      "Transportation       1.00      1.00      1.00         9\n",
      "  Volunteering       1.00      1.00      1.00        46\n",
      "\n",
      "   avg / total       1.00      1.00      1.00       194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from time import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "# Start running time\n",
    "t0 = time()\n",
    "\n",
    "# Build a pipeline that contains vectorizer, transform and classifier\n",
    "svm_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                            alpha=1e-3, random_state=42,\n",
    "                                            max_iter=5, tol=None)),\n",
    "                     ])\n",
    "\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Clean up testing data set\n",
    "clean_test_df = tweet_processor.process_tweets(y_test)\n",
    "\n",
    "# Test the classifier\n",
    "predicted = svm_clf.predict(clean_test_df)\n",
    "\n",
    "# Print running time for training and predicting\n",
    "print('Total running time', time() - t0)\n",
    "\n",
    "# Evaluate accuracy of the model\n",
    "accuracy = accuracy_score(y_test, predicted)\n",
    "print(accuracy)\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
