{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: TwitterAPI in /Users/nescobar/anaconda/envs/py36/lib/python3.6/site-packages\n",
      "Requirement already satisfied: requests in /Users/nescobar/anaconda/envs/py36/lib/python3.6/site-packages (from TwitterAPI)\n",
      "Requirement already satisfied: requests-oauthlib in /Users/nescobar/anaconda/envs/py36/lib/python3.6/site-packages (from TwitterAPI)\n",
      "Requirement already satisfied: oauthlib>=0.6.2 in /Users/nescobar/anaconda/envs/py36/lib/python3.6/site-packages (from requests-oauthlib->TwitterAPI)\n"
     ]
    }
   ],
   "source": [
    "#Install packages\n",
    "#!pip install python-twitter\n",
    "!pip install TwitterAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from TwitterAPI import TwitterAPI\n",
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = \"/Users/nescobar/Dropbox/Indiana/Social_Media_Mining/Project/smm2018/data/raw/csv/annotation/training_set_final_2.csv\"\n",
    "test_file = \"/Users/nescobar/Dropbox/Indiana/Social_Media_Mining/Project/smm2018/data/raw/csv/annotation/random_new_t.csv\"\n",
    "\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token_key = ''\n",
    "access_token_secret= ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#api = TwitterAPI(consumer_key, consumer_secret, auth_type='oAuth2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ProcessTweets:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._stopwords=set(list(punctuation)+['ATUSER','URL','IMG'])\n",
    "        \n",
    "    def get_data(query=\"\", source=\"file\", path=train_file, feed=\"search/tweets\",api=api, maxid=0, n=100):\n",
    "        try:\n",
    "            if source == \"file\":\n",
    "                harvey_df = pd.read_csv(path, encoding = 'ISO-8859-1')\n",
    "                return harvey_df\n",
    "            else:\n",
    "                if maxid == 0:\n",
    "                    return pd.read_json(json.dumps([t for t in api.request(feed, {'q':query,'count':n})]))\n",
    "                else:\n",
    "                    return pd.read_json(json.dumps([t for t in api.request(feed, {'q': query, 'count': n, 'max_id': maxid})]))            \n",
    "        except:\n",
    "            print(\"Error while getting data\")\n",
    "            return None\n",
    "    \n",
    "    def process_tweets(self, list_of_tweets):\n",
    "        processed_tweets=[]\n",
    "        for tweet in list_of_tweets:\n",
    "            processed_tweets.append((self._process_tweet(tweet)))\n",
    "        return processed_tweets\n",
    "    \n",
    "    def _process_tweet(self,tweet):\n",
    "        try:\n",
    "            # Unescape from HTML\n",
    "            #tweet = html.unescape(tweet)\n",
    "            #tweet = BeautifulSoup(tweet, 'lxml').get_text()\n",
    "            # 3a. Convert to lower case\n",
    "            tweet = tweet.lower()\n",
    "            # 3b. Replace links with the word URL \n",
    "            tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',tweet) \n",
    "            # 3c. Replace @username with \"AT_USER\"\n",
    "            tweet = re.sub('@[^\\s]+',' ',tweet)                \n",
    "            # 3d. Replace #word with word \n",
    "            tweet = re.sub(r'#([^\\s]+)',r'\\1',tweet)\n",
    "            # 3e. Replace images with the word IMG \n",
    "            tweet = re.sub(r'\\bpic.twitter.com\\s+', ' ', tweet)\n",
    "            # 3f Keep only words with letters\n",
    "            tweet = re.sub('[^a-zA-Z]',' ',tweet)\n",
    "            # 3g. Remove RT\n",
    "            tweet = re.sub(r'\\brt([\\b\\s])', ' ', tweet)\n",
    "            # Apply Lemmatization\n",
    "            lemmatizer = WordNetLemmatizer()           \n",
    "            tweet = [lemmatizer.lemmatize(word) for word in tweet.split() if word not in self._stopwords and len(word)>1]  \n",
    "            return (\" \".join(tweet)).strip()\n",
    "        except:\n",
    "            print(\"Error with tweet: \", tweet)\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process tweets from file\n",
    "tweet_processor = ProcessTweets()\n",
    "raw_tweets = tweet_processor.get_data()\n",
    "#[print(t) for t in raw_tweets['text'][:10]]\n",
    "cleaned_tweets = tweet_processor.process_tweets(raw_tweets['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 333 entries, 0 to 349\n",
      "Data columns (total 2 columns):\n",
      "text      333 non-null object\n",
      "target    333 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 7.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Create train DF \n",
    "train_df = pd.DataFrame(cleaned_tweets, columns=['text'])\n",
    "train_df['target'] = raw_tweets.label\n",
    "train_df=train_df.drop_duplicates('text')\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('high water', 104),\n",
       " ('water rescue', 58),\n",
       " ('flood water', 53),\n",
       " ('water level', 43),\n",
       " ('rising water', 20),\n",
       " ('water crossing', 20),\n",
       " ('low water', 18),\n",
       " ('low water crossing', 17),\n",
       " ('high water rescue', 16),\n",
       " ('corpus christi', 15),\n",
       " ('water rise', 14),\n",
       " ('stay safe', 14),\n",
       " ('turn around', 13),\n",
       " ('food water', 13),\n",
       " ('port lavaca', 13),\n",
       " ('lot water', 12)]"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top ngrams in tweets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "filter_file = \"/Users/nescobar/Dropbox/Indiana/Social_Media_Mining/Project/data/csv/filter_tweets_water.csv\"\n",
    "\n",
    "filtered_tweets = tweet_processor.get_data(path=filter_file)\n",
    "cleaned_filtered_tweets = tweet_processor.process_tweets(filtered_tweets['text'])\n",
    "\n",
    "vect = TfidfVectorizer(ngram_range=(2,5))\n",
    "summaries = \"\".join(cleaned_filtered_tweets)\n",
    "ngrams_summaries = vect.build_analyzer()(summaries)\n",
    "\n",
    "[bigram for bigram in Counter(ngrams_summaries).most_common(20) if \"harvey\" not in bigram[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes:  0.582501928858\n",
      "Logistic Regression:  0.690384973528\n",
      "Logistic Regression with SGD:  0.732677521483\n",
      "SVM:  0.348335860803\n",
      "SVM with SGD:  0.738425519462\n",
      "Total running time 1.1801233291625977\n"
     ]
    }
   ],
   "source": [
    "# Testing with different classifiers\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from time import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "# Start running time\n",
    "t0 = time()\n",
    "\n",
    "# Build a pipeline that contains vectorizer, transform and classifier\n",
    "\n",
    "# Multinomial NB\n",
    "clf = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\")),\n",
    "                 ('clf', MultinomialNB()),\n",
    "                ])\n",
    "scores = cross_val_score(clf, train_df.text, train_df.target, cv=5)\n",
    "print(\"Multinomial Naive Bayes: \", np.mean(scores))\n",
    "\n",
    "# Logistic Regression\n",
    "clf_lr = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\")),\n",
    "                 ('clf', LogisticRegression()),\n",
    "                ])\n",
    "scores = cross_val_score(clf_lr, train_df.text, train_df.target, cv=5)\n",
    "print(\"Logistic Regression: \", np.mean(scores))\n",
    "\n",
    "# Logistic Regression with SGD \n",
    "clf_lr_sgd = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\")),\n",
    "                 ('clf', SGDClassifier(loss='log', penalty='l1',\n",
    "                                            alpha=1e-3, random_state=42,\n",
    "                                            max_iter=100, tol=None)),\n",
    "                ])\n",
    "scores = cross_val_score(clf_lr_sgd, train_df.text, train_df.target, cv=5)\n",
    "print(\"Logistic Regression with SGD: \", np.mean(scores))\n",
    "\n",
    "# SVM\n",
    "clf = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\")),\n",
    "                 ('clf', SVC()),\n",
    "                ])\n",
    "scores = cross_val_score(clf, train_df.text, train_df.target, cv=5)\n",
    "print(\"SVM: \", np.mean(scores))\n",
    "\n",
    "# SVM with SGD (Stochastic Gradient Descent) \n",
    "clf = Pipeline([('tfidf', TfidfVectorizer(lowercase=True, stop_words=\"english\")),\n",
    "                 ('clf', SGDClassifier(loss='hinge', penalty='l1',\n",
    "                                            alpha=1e-3, random_state=42,\n",
    "                                            max_iter=100, tol=None)),\n",
    "                ])\n",
    "scores = cross_val_score(clf, train_df.text, train_df.target, cv=5)\n",
    "print(\"SVM with SGD: \", np.mean(scores))\n",
    "\n",
    "# Print running time for training and predicting\n",
    "print('Total running time', time() - t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.741741741742\n",
      "{'clf__alpha': 0.001, 'clf__class_weight': None, 'clf__fit_intercept': True, 'clf__max_iter': 100, 'clf__penalty': 'l1', 'clf__warm_start': True, 'tfidf__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "# Parameter tuning with Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'tfidf__ngram_range' : [(1,1),(1,2)],\n",
    "             'clf__alpha': (1e-1, 1e-3, 1e-5),\n",
    "             'clf__max_iter': (10,50,100),\n",
    "             'clf__penalty': ('l1','l2','elasticnet'),\n",
    "             'clf__fit_intercept': (True,False),\n",
    "             'clf__class_weight': (None,'balanced'),\n",
    "             'clf__warm_start': (True, False)\n",
    "             }\n",
    "\n",
    "gs_clf = GridSearchCV(clf_lr_sgd, parameters, n_jobs=-1)\n",
    "gs_clf.fit(train_df.text, train_df.target)\n",
    "\n",
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time 0.33081698417663574\n",
      "['donation' 'other' 'relocation' 'volunteering']\n",
      "'Respiratory problems, pneumonia, headaches, nausea and dizziness – all are side effects that some first responders and community members cited after being exposed to a toxic mix of chemicals at an Arkema chemical plant during # HurricaneHarvey. http://trib.it/9W pic.twitter.com/vXWuj3dElX' => [ 0.31938091  0.37897407  0.1360919   0.16555311]\n",
      "'# HurricaneHarvey update. # TexasStronghttps://gov.texas.gov/news/post/commission-to-rebuild-texas-after-hurricane-harvey-update-issue-20 …' => [ 0.33245161  0.32133849  0.17388151  0.17232839]\n",
      "'# Telehealth in Times of Natural Disaster. https://jamanetwork.com/journals/jama/fullarticle/2677450 … # HurricaneHarvey' => [ 0.33605348  0.31398568  0.17576539  0.17419545]\n",
      "'Great speech by @ tedcruz honoring # BorderPatrol agents in Edinburg, Texas. # ToughAsTexas # BorderSecurity # JavierVegaJr. # NewtonAzrak # HurricaneHarvey @ BPUnionhttps://twitter.com/dailysignal/status/981305508538839041 …' => [ 0.37087243  0.33167936  0.10520414  0.19224407]\n",
      "'7 months after # HurricaneHarvey, Houston is still recovering. This week @ Oath employees from around the globe are volunteering their time and effort to help rebuild houses in the community. Check out some shots from Day 1! @ SBPUSA # OathforGoodpic.twitter.com/o7bGRyivGW' => [ 0.37851001  0.33994837  0.1404792   0.14106243]\n",
      "'Poem written in 5 minutes about how I feel after my childhood home was gutted and destroyed during # HurricaneHarvey No edits or filters! # writersblock # PoemADay' => [ 0.24144261  0.57753748  0.12628126  0.05473865]\n",
      "'ICYMI: U.S. Sen. Ted Cruz formally launched his re-election bid, emphasizing # HurricaneHarvey recovery. \"Texans are strong. Texans are free...Texans are tough.\" http://bit.ly/2q12kKO pic.twitter.com/faStkNkoIr' => [ 0.31797606  0.35088865  0.16631039  0.1648249 ]\n",
      "'Yep...Thank You # HurricaneHarvey https://twitter.com/bored_teachers/status/980860873869791232 …' => [ 0.26882347  0.44180037  0.14533716  0.144039  ]\n",
      "'The third in our # HurricaneHarvey series looks at coastal resilience and infrastructure and how a multifaceted approach is needed @ Jamesjdrew @ chrondigger @ edward_klumphttps://www.edf.org/684' => [ 0.33446272  0.31723304  0.17493338  0.17337086]\n",
      "'Third post in our # HurricaneHarvey series looks at how a multifaceted approach could help strengthen coastal resilience before the next storm comes @ mihirzaveri @ mizzousundevil @ neenareports @ KiahCollier https://www.edf.org/684' => [ 0.30586557  0.40997631  0.12833625  0.15582186]\n",
      "'An \"affordable home\" that floods every year is not really affordable. We need Chapter 19 standards that fit the reality Houston faces # hounews # HurricaneHarvey' => [ 0.28529773  0.4674523   0.14921871  0.09803126]\n",
      "'Turning disasters into opportunities… https://goo.gl/sjXwo6 @ ConversationUS Look for related EHP News content, coming soon! # HurricaneHarvey # DisasterRestoration # Resilience' => [ 0.31176106  0.34851459  0.17062419  0.16910016]\n",
      "'Help care for more than 30 dogs & puppies just rescued from overcrowded shelters after # HurricaneHarvey. Make an urgent gift to @ AnimalLeague:https://bit.ly/2unp3FM' => [ 0.0688914   0.16945974  0.69681934  0.06482953]\n",
      "\"Freaking # HurricaneHarvey damage repairs for us were twice what we got from FEMA. The lists for charity assistance on home repairs are very long. There's nothing for vehicle replacement. We had almost 4ft of H2O and lost 4 used cars too.\" => [ 0.31909133  0.34861196  0.16689371  0.16540301]\n",
      "\"I could use prayers to remove the # HurricaneHarvey efflouressence from the floor in one of or bedrooms. I've never experienced that stuff before. I thought that it was mold. One of the many scary things with this tragic event.\" => [ 0.26304076  0.51827586  0.13757769  0.08110569]\n",
      "'A multifaceted approach could strengthen Texas’ coastal resilience before the next # HurricaneHarvey. Here’s how. https://www.edf.org/6XS by @ H2Owitch @ KateZerrenner' => [ 0.36169124  0.3388709   0.11195292  0.18748494]\n",
      "'Thanks to our educational foundation, we are able to offer FREE # HurricaneHarvey regional CLEs in Houston, Beaumont, & Victoria this summer. For details, visit https://www.tdcaa.com/node/15283' => [ 0.32250011  0.33976994  0.19186741  0.14586254]\n",
      "'Public DIdn’t Get the Whole Story of # HurricaneHarvey’s # Toxic Impact https://buff.ly/2Ii1L64 @ EnvNewsBits # Houston # pollution' => [ 0.32948175  0.3208451   0.17232819  0.17734496]\n",
      "'Robert Trout supported shelters during # HurricaneHarvey where he traveled daily, working 12 hour shifts to serve more than 200 shelter residents each day at a local high school. # MRCAwards @ FBMRC . @ HPPInfo' => [ 0.02690353  0.05357642  0.81682949  0.10269056]\n",
      "'Chicken Fried Chicken - https://buff.ly/2FU3COR via @ MooshuJenne in support for # HurricaneHarvey relief by # foodbloggers4txpic.twitter.com/ky2rflWbbv' => [ 0.38525826  0.25446532  0.17813175  0.18214466]\n",
      "'Our next Responder recipient Robert Trout of the Fort Bend County MRC in Texas is a registered nurse and has been a volunteer since 2016. He took on a leadership role during 2017 responses to a TB outbreak and # HurricaneHarvey. # MRCAwards @ FBMRC . @ HPPInfo' => [ 0.01444243  0.17560059  0.04128783  0.76866915]\n",
      "'The selfless service to # HurricaneHarvey victims on Obamacare repeal. We must \"finish the original Mission Control desk of' => [ 0.33438031  0.31740127  0.17489028  0.17332815]\n",
      "'Disaster: # HurricaneHarvey Occurred 221 days ago Updates: -The impact of toxic spills appear to be worse than expected (https://ti.me/2Go6qGD ) -Local officials are struggling to raise funds for flood relief projects (https://lat.ms/2I9Zt9p )' => [ 0.49028523  0.21194303  0.14000654  0.1577652 ]\n",
      "'DON thanks the students from @ TheWebbSchools who volunteered on # hurricaneharvey projects instead of going to Belize for # springbreak.pic.twitter.com/oK77s6NNe4' => [ 0.22779344  0.48931211  0.14208177  0.14081269]\n",
      "'A multifaceted approach could strengthen Texas’ coastal resilience before the next # HurricaneHarvey. Here’s how. https://www.edf.org/68k by @ H2Owitch @ KateZerrenner' => [ 0.36169124  0.3388709   0.11195292  0.18748494]\n",
      "\"Did you know # ACAMHouston's network of # ministries serve 274,000 annually in the Greater # Houston Region? Many of whom are still working to recover from # HurricaneHarvey. # Volunteer or # Donate today http://www.acamweb.org # nonprofits # HelpEachOther # nonprofitpic.twitter.com/Z2BsCE0r1g\" => [ 0.05665646  0.05342655  0.04187311  0.84804388]\n",
      "'Together we can and will recover! # HurricaneHarvey # RebuildingHope # HabitatHammersBackhttps://twitter.com/txglo/status/980894217516888064 …' => [ 0.37291468  0.2387378   0.19504483  0.19330268]\n",
      "'HOUSTON CLONES!!! WRL race at MSR Houston on April 21 IS A MUST SEE EVENT where this # HurricaneHarvey car is going to be raced & auctioned for Harvey Relief! https://twitter.com/Jalopnik/status/981204603160203266 …' => [ 0.44525897  0.24744524  0.1973753   0.10992048]\n",
      "'Although it has been just over 7 months since # HurricaneHarvey hit the Texas coast, residents in the region are still recovering. A FEMA/SAMHSA grant program - Texans Recovering Together ( @ coastalbendtrt) - offers crisis counseling services # mentalhealthhttps://www.victoriaadvocate.com/news/features/nonprofits/education-agency-takes-lead-on-harvey-crisis-counseling/article_25f9878c-36ac-11e8-be9e-2793fd19c180.html …' => [ 0.35950213  0.29553636  0.19454874  0.15041276]\n",
      "'Learn from the experts # casemanagers that survived through # HurricaneHarvey and how you can prepare for the unexpected. https://zurl.co/L8vqD # disasters # healthare # RN # nursing # caremanager' => [ 0.32272208  0.34120019  0.1687927   0.16728503]\n",
      "'More than six months after # HurricaneHarvey and there is still work to be done! Our Houston Green Team and 59 Northeast District partnered with @ AvenueCDC to help a local family renovate the exterior of their home in the Northside community. # houstonstrong # wfvolunteerspic.twitter.com/XgkuZRVEf9' => [ 0.37378157  0.24310313  0.19380435  0.18931096]\n",
      "'# RT @ TexasGOP: RT @ txglo: The @ txglo and Commissioner @ georgepbush take part in our weekly call with local officials and @ fema as we coordinate our ongoing work of helping Texans recover from # HurricaneHarvey. pic.twitter.com/V0iB0tdSL2' => [ 0.31517613  0.27583293  0.16484595  0.24414499]\n",
      "'Survey by @ Health4Texas & @ KaiserFamFound found 45% of those who suffered losses by # HurricaneHarvey say they are not getting the help they need to recover. http://bit.ly/2o0e8MA # needhelp # HoustonStrong # TexasStrong # ACAMHouston # HelpEachOther' => [ 0.38742299  0.29199422  0.11665323  0.20392956]\n",
      "'# hurricaneharvey Government’s fault or Mother Nature?? https://www.nytimes.com/2018/03/19/us/harvey-texas-flooding.html?smid=ig-nytimes&smtyp=cur&smvar=story …' => [ 0.37291468  0.2387378   0.19504483  0.19330268]\n",
      "'RT @ TREDIS_software: # Transportation Planners Hear The Concerns Of People Recovering From Harvey https://www.houstonpublicmedia.org/articles/news/transportation/2018/03/30/276281/transportation-planners-hear-the-concerns-of-people-recovering-from-harvey/ … # hurricanes # HurricaneHarvey # disasterrecovery # infrastructure' => [ 0.37683502  0.34361727  0.12735902  0.15218869]\n",
      "'Great reporting by @ HoustonChron demonstrating how widespread the problem really is. @ mihirzaveri @ mizzousundevil @ davidhunn # HurricaneHarvey # Houston # Htxhttps://twitter.com/mizzousundevil/status/980820460043689984 …' => [ 0.33548785  0.34166754  0.17546955  0.14737506]\n",
      "\"Check out this powerful photo on @ RETREETorg's instagram. We are proud to be working with them this May! # redefinedisasterrelief # volunteer # HurricaneHarvey More information on our service project here: https://www.youcaring.com/nashvilledisasterreliefteam-1021183 …https://twitter.com/RETREETorg/status/979378514733469696 …\" => [ 0.01472025  0.20918716  0.0383057   0.7377869 ]\n",
      "'More # HurricaneHarvey fallout - # chemicalspill toxicity was underreported to the public, according to new @ AP reporting: https://thebea.st/2uycyqK # resilience' => [ 0.24348671  0.57188571  0.12735038  0.0572772 ]\n",
      "'Texas needs a multifaceted approach to improve coastal resilience before the next # HurricaneHarvey hits. Here are some ideas how to do that https://www.edf.org/684' => [ 0.34892385  0.34837052  0.12183874  0.18086689]\n",
      "\"Geospatial analysis can reveal impacts to eco-systems following # HurricaneHarvey's record # rainfall & # flooding. Learn more about HARC's # geospatial & # analytics services at http://GIS.HARCresearch.org . Imagery by @ TNRIS & @ hgaccog # environment # floods # GIS # greeninfrastructure @ Esripic.twitter.com/k2NhzOezEW\" => [ 0.27797086  0.46877769  0.14538655  0.1078649 ]\n",
      "'Recovery is a work in progress on the # Texas coastline http://atty.es/Cdl930jgWW8 # HurricaneHarvey pic.twitter.com/6rlxTzlleF' => [ 0.35423621  0.34449049  0.11765273  0.18362058]\n",
      "'# ArchitecturalFiction, # HurricaneHarvey finally makes landfall. # AllisonRising. # Houston http://bit.ly/2G1boW1 pic.twitter.com/jxRPXLCrBc' => [ 0.38420331  0.31253682  0.16463428  0.13862559]\n",
      "'How a multifaceted approach could strengthen Texas’ coastal resilience before the next # HurricaneHarvey https://www.edf.org/68k' => [ 0.36169124  0.3388709   0.11195292  0.18748494]\n",
      "\"The results of an engineering study provided no easy solutions to # Richwood's flooding woes, but findings vindicate actions city officials took in the wake of # HurricaneHarvey, Councilwoman Lauren LaCount said. # BrazoriaCounty http://thefacts.com/news/article_09a2f4d3-3a4e-5675-b94b-71f256522574.html#utm_campaign=blox&utm_source=twitter&utm_medium=social …\" => [ 0.34007899  0.35673401  0.17787085  0.12531615]\n",
      "'The @ txglo and Commissioner @ georgepbush take part in our weekly call with local officials and @ fema as we coordinate our ongoing work of helping Texans recover from # HurricaneHarvey. pic.twitter.com/XtVur7TcWq' => [ 0.31517613  0.27583293  0.16484595  0.24414499]\n",
      "'# Houston is home to nation’s largest energy corridor, 500 chemical plants, 10 refineries & 6,000 miles of oil, gas & chemical pipelines. # Texas can’t protect fm future disasters until understands environmental impact # HurricaneHarvey. # HouRecovers # TXlegehttps://www.houstonchronicle.com/opinion/editorials/article/Hurricane-Harvey-s-toxic-legacy-Editorial-12795262.php?utm_campaign=twitter-premium&utm_source=CMS%20Sharing%20Button&utm_medium=social …' => [ 0.34425364  0.36465186  0.13466387  0.15643063]\n",
      "'# CruzCrew, volunteers from Mexican drug cartels to help # HurricaneHarvey' => [  6.58913518e-05   3.31256301e-02   1.87367633e-03   9.64934802e-01]\n",
      "'After # HurricaneHarvey, there was a ‘second storm’ of air pollution, according to state reports: http://ow.ly/Cn8k30jh6oa # HarveyRecovery # disasterpic.twitter.com/yHyNdSZrTo' => [ 0.20557266  0.61211728  0.120263    0.06204706]\n",
      "'Interested in # HurricaneHarvey @ RebuildTXtoday and the efforts to engage with vulnerable populations? @ TAMULawSchoolpic.twitter.com/L3DYoIxYoe' => [ 0.43392686  0.23131895  0.16812796  0.16662623]\n",
      "'Same' => [ 0.30584097  0.37566103  0.1599634   0.1585346 ]\n",
      "'The people injured following HurricaneHarvey has a bullet getting out with the early morning to like the only the brainwashed' => [ 0.26073181  0.18648678  0.35460843  0.19817298]\n",
      "'# HurricaneHarvey struck way back in August. Since then, # hurricanes in # Florida and across # LatinAmerica have devastated communities. # IRUSA responded to the call for help each time, providing emergency aid - http://bit.ly/2phMStJ pic.twitter.com/8T03AYdMzJ' => [ 0.4024347   0.4076797   0.06602668  0.12385892]\n",
      "\"Don't be a victim of those great deals coming in from Houston # HurricaneHarvey\" => [ 0.22363196  0.51142907  0.14408307  0.1208559 ]\n",
      "'Hello Houston! @ Oath has arrived. We’re proud to bring together 27 Oath volunteers to partner with @ SBPUSA to rebuild homes and communities in the Houston area that have been devastated by # HurricaneHarvey. # OathForGoodpic.twitter.com/3lGzWdNydg' => [ 0.03048122  0.31140707  0.08875454  0.56935717]\n",
      "'. @ tedcruz puts # HurricaneHarvey response front and center in a new video to launch his campaign for # txsen called \"Tough as Texas\"https://www.youtube.com/watch?v=cuwZKmdo50o …' => [ 0.30977336  0.41150116  0.11815251  0.16057298]\n",
      "'Hey Aaron, this seems like forever ago but here to tell you that you won my (long delayed by procrastination) contest. Thanks again for your donation to help # HurricaneHarvey relief efforts. Let me know what jersey you want and DM me your address. Post a pic when you get it. https://twitter.com/ajhauk/status/902553977945866241 …' => [ 0.75197835  0.02472693  0.1259494   0.09734533]\n",
      "'# HOUSTONtexas HOW ABOUT NOT OPENING THE FLOOD GATES ON HOUSTON AT 2:30 AM RIGHT AFTER # HurricaneHARVEY MOVES OUT?? First steps in creating flood warning system https://youtu.be/gyyR3aaoaBo via @ YouTube' => [ 0.20766168  0.55252357  0.18073104  0.0590837 ]\n",
      "'“Texas has had its share of adversity,” Cruz said as he talks about # HurricaneHarvey' => [ 0.34457088  0.22927539  0.24754324  0.17861049]\n",
      "'New post (BuildAid Partners with David Weekley Homes to Rebuild ...) has been published on , # GreaterHoustonBuildersAssociationBuilders # HomeRepairs # Homeowners # HurricaneHarvey # Stlproperties # STLRealEstateNews , # STLRealEstateNews , More Details - https://goo.gl/G4tB3d pic.twitter.com/JBx6tKvqHO' => [ 0.27823683  0.32944324  0.24809411  0.14422582]\n",
      "'The U.S. Census Bureau is going to have a tough time thanks to # HurricaneHarvey https://twitter.com/TexasTribune/status/980847350930395136 …' => [ 0.33053402  0.32525302  0.17287856  0.1713344 ]\n",
      "'During # HurricaneHarvey, first responders & neighborhoods around an Arkema chemical plant in Crosby say they were exposed to a “toxic cloud” of chemicals. Some argue the blame isn’t completely on Arkema – it’s on lax chemical regulations. http://trib.it/9W' => [ 0.3208386   0.37924358  0.1336091   0.16630872]\n",
      "\"“Texas is better when your economy is strong.” @ GovAbbott shared this message with local leaders and business members of the Rockport-Fulton Chamber in a video. Issue #20 of the Governor's Commission To Rebuild Texas # HurricaneHarvey Update http://ow.ly/dXBb30jhjFh pic.twitter.com/rqmPFGLFRo\" => [ 0.29897333  0.41205343  0.09307256  0.19590068]\n",
      "'HPE employees continue to support the # HurricaneHarvey recovery effort. Recently our happy volunteers planted over 400 plants at @ buffalobayou park. Who knew we had such a green thumb?! # houstonstrong # HPEGives @ SylvesterTurner' => [ 0.04153179  0.22801395  0.05781902  0.67263523]\n",
      "'Listen to @ EdHubbard and @ SteveParkhurst on The Price of Business as they discuss Renewing the American Neighborhood and our new case study on Houston after # HurricaneHarvey. # Localism http://www.renewingamericanneighborhood.com/ed-hubbard-and-steve-parkhurst-on-price-of-business/ …pic.twitter.com/evCpiYGdW4' => [ 0.33512756  0.34208503  0.17528111  0.14750629]\n",
      "'Thank you Southwest Christian High School! We appreciate your hard work in Texas and are thankful for you all! # thirstmissions # thirstmissionstx # texas # texasgulfcoast # hurricaneharvey # hurricanerelief # missiontrip # serviceprojectspic.twitter.com/bmywyl4OWf' => [ 0.38078279  0.36122117  0.08834981  0.16964623]\n",
      "'This 5-part series – “Stacy Lewis: Winning for Houston” – chronicles the moments when Stacy Lewis won the LPGA Portland Classic and decided to donate – and match – her winnings to # HurricaneHarvey relief efforts. http://bit.ly/2pZjFDZ pic.twitter.com/RsXCk0lf3L' => [ 0.74659214  0.05786506  0.06344187  0.13210093]\n",
      "'Day 218: Social media saved our lives and continues to connect relief efforts in the wake of # Hurricaneharvey seven months later. All my love and gratitude to this group of students through Unique Student... https://www.facebook.com/katiemehnert/posts/10156366879154374 …' => [ 0.39301237  0.26487587  0.17010618  0.17200559]\n",
      "'\"Case manager leaders found that preparing staff to be mobilized or available via phone was crucial during Hurricane Harvey.\" https://zurl.co/5V95w # Casemanager # healthcare # hurricaneharvey' => [ 0.25845357  0.63568349  0.0728015   0.03306145]\n",
      "\"Life after # HurricaneHarvey Children's Museum donates 250 books to Northeast Campus -A special thank you goes to Ms. Gina Gaston and KTRK Channel 13 # GinaGaston # KTRK # ABC13 # Varnett # bookdrive # Students https://northeast.varnett.org/apps/news/article/848010 …pic.twitter.com/gB2MOza2QO\" => [ 0.28906468  0.40554217  0.15338158  0.15201157]\n",
      "\"@ cmhouston's Museum donates 250 books to Northeast Campus -A special thank you goes to Ms. Gina Gaston and KTRK Channel 13 # ABC13 # GinaGaston @ GinaGaston13 # varnett # HurricaneHarvey # bookdrive # HoustonChildrensMuseum https://northeast.varnett.org/apps/news/article/848010 …pic.twitter.com/2ucvxSAxiX\" => [ 0.28532347  0.41222961  0.15190186  0.15054506]\n",
      "'WATCH: With new # ToughAsTexas slogan, @ tedcruz launches re-election bid emphasizing # HurricaneHarvey recovery. @ SenTedCruz is defending his bid for a second term against U.S. Rep. @ BetoORourke. http://bit.ly/2pZXoVT pic.twitter.com/28FVVODnhi' => [ 0.32104098  0.34463196  0.16791344  0.16641362]\n",
      "'Brutal choice in Houston: Sell home at a loss or face new floods http://ow.ly/4DsD30jgNzO # hurricaneharvey' => [ 0.28071603  0.48399394  0.14682235  0.08846768]\n",
      "'@ HPE employees continue to support the # HurricaneHarvey recovery effort. Recently our happy volunteers planted over 400 plants at @ buffalobayou park. Who knew we had such a green thumb?! # houstonstrong # HPEGives @ SylvesterTurnerpic.twitter.com/PWG8HpJF2t' => [ 0.04153179  0.22801395  0.05781902  0.67263523]\n",
      "'TX: Help Remains after Disaster Recovery Centers Closed Help is still a tap away on the FEMA app A phone call away (800)621-3362 or (800)462-7585 (TTY) A mouse click away at: http://fema.gov/DRC or http://DisasterAssistance.gov # HurricaneHarvey pic.twitter.com/FddHuR7mXs' => [ 0.38113206  0.29808559  0.14711712  0.17366523]\n",
      "'This updated infographic shows the overall progress of recovery from # HurricaneHarvey. Partnerships with local communities have been key to helping survivors. @ fema and @ txglo remain committed to making sure # Texans receive assistance to help with their recovery.pic.twitter.com/4nPvCEhtAs' => [ 0.35090268  0.2831032   0.15369995  0.21229417]\n",
      "'We’re in the field collecting samples today! Many of the samples we collect are in areas that were flooded by # HurricaneHarvey. We’ analyze the soil to determine what contaminants might have been spread in with the hurricane floodwaters. # thisispublichealth # NPHWpic.twitter.com/1W4906n29Y' => [ 0.33892565  0.42008419  0.10199211  0.13899805]\n",
      "\"My silly mind after reading this tweet went to my personal struggle. We've for efflouressence on one bedroom floor thanks to # HurricaneHarvey. Researching best way to remove and treat it...\" => [ 0.30839082  0.4342852   0.09746765  0.15985633]\n",
      "'\"Houston-Area Leaders Seeking More Than $1B for Flood Control\" - via @ usnews https://www.usnews.com/news/best-states/texas/articles/2018-03-27/houston-area-leaders-seeking-more-than-1b-for-flood-control … # HurricaneHarvey # TexasFlood # SevereWx' => [ 0.27222223  0.52236968  0.13713296  0.06827513]\n",
      "\". @ SenTedCruz formally kickstarted his re-election campaign today with a focus on Texas' heroic response to # HurricaneHarvey and his involvement in the recovery. Via @ TexasTribune:http://www.tinyurl.com/yclyexns\" => [ 0.34006913  0.35459984  0.12905405  0.17627698]\n",
      "'Missing this goon a bit extra today # puppy # furbaby # montagne # HurricaneHarvey # dogmompic.twitter.com/ETbZI49bFS' => [ 0.32893714  0.32851286  0.17204335  0.17050665]\n",
      "'Tens of thousands of people living in coastal communities were displaced by # HurricaneHarvey last year. That’s going to make them hard for the U.S. Census Bureau to count. # txlege http://bit.ly/2I0vqk5 pic.twitter.com/KBSWCOHdtC' => [ 0.16254192  0.37592623  0.29931931  0.16221253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Disaster: # HurricaneHarvey Occurred 220 days ago Local charities helping the recovery: - @ HoustonFoodBank - @ HouUnitedWay - @ HoustonHumane - @ Montrose_Center - @ commitforlife - @ YMCAHouston' => [ 0.31441937  0.29560115  0.16445015  0.22552933]\n",
      "'This 5-part series – “Stacy Lewis: Winning for Houston” – chronicles the moments when Stacy Lewis won the LPGA Portland Classic and decided to donate – and match – her winnings to # HurricaneHarvey relief efforts. @ espnW # hurricaneharvey @ KPMGGolf http://bit.ly/2H6r4ZO pic.twitter.com/A107fmX4Ze' => [ 0.72892973  0.06007525  0.07233846  0.13865656]\n",
      "'Following analysis of the damage and # flood impacts of # HurricaneHarvey, # Houston’s Dept of # PublicWorks is recommending that all new structures in the 100- and 500-year floodplains be elevated to 2 ft above the 500-year flood elevation. http://ow.ly/tbgw30jeHtn # infrastructure' => [ 0.26460626  0.51027968  0.1383965   0.08671756]\n",
      "'Could financial assurance mandates help avoid future industrial disasters like # HurricaneHarvey? Find out at ELPAR on 4/6. Learn more and register at http://goo.gl/tFecqe pic.twitter.com/OYd4ZbQ0HJ' => [ 0.39774762  0.26893214  0.15255887  0.18076137]\n",
      "'A fundamental decision — to stay or go — is being faced by homeowners in the areas hit by # HurricaneHarvey https://buff.ly/2JbhlC0 via @ nytimespic.twitter.com/Z3bZ2aOnnD' => [ 0.32020559  0.39802544  0.16011854  0.12165043]\n",
      "\"South Texas tourism season is nearing. What's the status of # Rockport & # PortAransas since # HurricaneHarvey I've seen conflicting info\" => [ 0.37382031  0.30950868  0.13302458  0.18364643]\n",
      "'# Houston # HurricaneHarvey pups land in # SanDiego, hopeful for # adoption! http://bit.ly/2EatkMc @ HWAC @ OPAtweeters # adoptadog' => [ 0.34877616  0.3262206   0.18241971  0.14258353]\n",
      "'# HurricaneHarvey damaged more than 204,000 homes and apartment buildings in Harris County, almost 3/4 of them outside the federally regulated 100-year flood plain, leaving tens of thousands uninsured and unprepared. My latest w/ @ davidhunn and @ mihirzaverihttps://www.houstonchronicle.com/news/article/In-Harvey-s-deluge-most-damaged-homes-were-12794820.php …' => [ 0.2935356   0.4895017   0.09214473  0.12481796]\n",
      "'# HurricaneHarvey spawned nearly 4,000 tons of unpermitted air # pollution from 75 industrial sources in the Houston and Port Arthur areas and beyond:https://www.houstonchronicle.com/news/houston-texas/houston/article/After-Harvey-a-second-storm-of-air-12795260.php?utm_campaign=twitter-premium&utm_source=CMS%20Sharing%20Button&utm_medium=social …' => [ 0.32840173  0.40846544  0.16383288  0.09929996]\n",
      "'Kicking off the 2018 # txsen campaign with a stop in Beaumont to talk about # HurricaneHarvey and the people here who are # ToughAsTexaspic.twitter.com/f5xxv5T3la' => [ 0.29075931  0.32001148  0.20685434  0.18237488]\n",
      "'Why storm recovery should address pre-storm problems like health and housing # HurricaneHarvey http://bit.ly/2GXAUgr' => [ 0.21658835  0.50073055  0.15038172  0.13229938]\n",
      "\"We're spending National Public Health Week looking back at public health during Hurricane Harvey. Today, an overview of the catastrophic impact the the storm. Check back here throughout the week for more. # NPHW # publichealth # harvey # hurricaneharvey pic.twitter.com/RuOSI3yLUE\" => [ 0.31722375  0.43540388  0.1269304   0.12044197]\n",
      "'ICYMI: We are some of the lucky ones, house repaired post-Harvey though still working with @ SBAgov for repair $ (they keep losing our file - sigh). Do you have a # HurricaneHarvey story to tell? Still need help? Let me know. dayna@daynasteele36.com pic.twitter.com/ypyE2DjoYd' => [ 0.29071975  0.17387247  0.39491331  0.14049447]\n",
      "'What the Insurance Companies Won’t Tell You About Your # HurricaneHarvey claims: http://ow.ly/wpYT30jdI60 pic.twitter.com/Naaa6pzxFL' => [ 0.29522349  0.41592584  0.15441016  0.13444051]\n",
      "\"Thanks to the outpouring of support we received last year, we're able to support organizations working in Harvey-impacted areas. We've awarded two additional grants from the # HurricaneHarvey Recovery Fund to long-term recovery groups: http://ow.ly/uHnp30jbQmy # CDP4Recovery\" => [ 0.32486024  0.38275478  0.16469026  0.12769472]\n",
      "'In response to major floods that devastated # Houston, the United States # ArmyCorps of Engineers built # Barker in the 1940s to protect the downtown area. # HurricaneHarvey # Harvey # Katy # KatyTX # CanyonGate # CincoRanch # FortBend # FtBend # TXlege # HouRecovershttps://nyti.ms/2FTNGyZ' => [ 0.2856656   0.45103725  0.14879946  0.11449768]\n",
      "'# Katy neighborhood on land designed to be flooded.Part of reservoir built by # ArmyCorps in 1940s to prevent catastrophic flooding downtown, fact developers did little to publicize when they built # CanyonGate 1990s # Harvey # HurricaneHarvey # FortBend # TXlegehttps://nyti.ms/2uwOW6b' => [ 0.34025511  0.34513574  0.18375105  0.13085809]\n",
      "'When # HurricaneHarvey struck Houston, floodwaters swept through Eileen and Jeff Swanson’s two-story brick home, blanketing the first floor in muck and nearly destroying a domestic existence 12 years in the making. @ abscribe @ nytimeshttps://www.nytimes.com/2018/03/30/us/hurricane-harvey-flooding-canyon-gate.html?smid=tw-nytimes&smtyp=cur …' => [ 0.32352808  0.33474971  0.16921426  0.17250795]\n",
      "\"It's Raining it's Pouring: How You Can Help Texans http://bit.ly/2xoOzXr # harveyflood # homeschooling # hurricaneharvey\" => [ 0.46953156  0.19323216  0.14290355  0.19433273]\n",
      "\"Any word on when my husband will be paid for the 2 weeks he spent in Houston (away from our family) for # HurricaneHarvey He's a State police officer. It has been over 6 months...which is ridiculous.\" => [ 0.30798271  0.22086157  0.33154803  0.1396077 ]\n",
      "'Never a dull moment @ StThomasEpisHou. 2 of 3 services began in darkness - one timed for sunrise, the main service for a power failure. If # HurricaneHarvey taught us anything, it’s definitely go with the flow! # Easter2018pic.twitter.com/gd0xrHY2j4' => [ 0.32138588  0.3439279   0.16809383  0.1665924 ]\n",
      "'As spring rain begins to fall we are working to accelerate flood mitigation projects around Harris Co. like this funding to kick-start the dredging of the San Jacinto River. # txlege # HurricaneHarvey # Kingwoodhttps://communityimpact.com/houston/lake-houston-humble-kingwood/editors-pick/2018/03/16/abbott-approves-5-million-state-funding-san-jacinto-river-dredging-flood-control-study/ …' => [ 0.2863923   0.44231977  0.1497912   0.12149673]\n",
      "'Tough times for Texas ranchers # txag # hurricaneharvey # txpoliticshttps://www.mystatesman.com/news/state--regional/seven-months-later-hurricane-harvey-still-hammers-east-texas-ranchers/xckUmCm4nRgYMFDUh1SBtM/ …' => [ 0.37387689  0.32926304  0.10305863  0.19380145]\n",
      "\"Flint, Michigan US Virgin Islands,and Puerto Rico are still suffering! Victims of # HurricaneHarvey are still struggling in TX! Yet here we are in the firm grip of a reality show residing at 1600 Pennsylvania Ave. in DC! Here's a partial peak: https://twitter.com/JohnRMoffitt/status/980531889302835200 …\" => [ 0.34160948  0.33016241  0.16485028  0.16337783]\n",
      "'Were you reminded during # LDSconf # SisAburto talk of the experiences during # HurricaneHarvey? # MormonHands were truly one people to get through that tragedy. ICYMI https://www.lds.org/general-conference/2018/04/media/session_4_talk_5/5761842437001?lang=eng …pic.twitter.com/LXNa7WlP2s' => [ 0.28828344  0.31192726  0.21402783  0.18576147]\n",
      "'Thank you to all the volunteers who have joined us to help hurricane victims in # Texas after # HurricaneHarvey. We are blessed to serve alongside you!' => [ 0.00662995  0.17492345  0.00563291  0.81281368]\n",
      "'If anyone has any critical relief supplies to help the areas damaged by # HurricaneHarvey, I have a charter bus with an empty cargo hold leaving AT&T Stadium going to Houston with my staff to work the home opener of... https://www.facebook.com/GeraldKern/posts/10155366922921500 …' => [ 0.40785132  0.3358438   0.18135717  0.07494771]\n",
      "'Hey there Marley! Big thanks to the @ TXStateAquarium for evacuating him to their facility when # HurricaneHarvey hit. pic.twitter.com/emy023Jze7' => [ 0.32516201  0.33621936  0.17006885  0.16854978]\n",
      "'Marley the spotted # MorayEel is in good hands at the @ TXStateAquarium He will be living the good life in there until the # Rockport aquarium is rebuilt post # HurricaneHarvey pic.twitter.com/Wf0lylNdOs' => [ 0.09518484  0.63668999  0.134664    0.13346117]\n",
      "'Big crowd expected. Am leaving a great game after the Oval Office w/senior U.S. HORRIBLE! # HurricaneHarvey' => [ 0.32316161  0.34030294  0.16902258  0.16751286]\n",
      "'# Recovery is a work in progress on the Texas coastline - https://www.washingtonpost.com/national/recovery-is-a-work-in-progress-on-the-texas-coastline/2018/03/31/16c8cb44-2d22-11e8-8688-e053ba58f1e4_story.html … via http://news.google.com # HurricaneHarvey # justice' => [ 0.35423621  0.34449049  0.11765273  0.18362058]\n",
      "'Trevor McDonald from # TheBookofJohnGray lost everything in # HurricaneHarvey & @ RealJohnGray gave him a watch? How about a check? Gray & @ JoelOsteen could do more for their youth minister! # LakewoodChurch # BecometheBridgeforTrevor # PasstheOfferingPlate' => [ 0.32478642  0.33698609  0.1698724   0.16835509]\n",
      "'Their story — of how a church & synagogue came together through crisis & upheaval, how they formed one family in the midst of fear and uncertainty, how faith was shaken and restored — is the story of this season. # Easter # Passover # HurricaneHarvey # Harveyhttps://www.houstonchronicle.com/life/houston-belief/article/Through-Harvey-church-and-temple-come-together-12795015.php?utm_campaign=twitter-mobile&utm_source=CMS%20Sharing%20Button&utm_medium=social …' => [ 0.27077708  0.47462282  0.16718849  0.08741162]\n",
      "'This is an # excellent article on how various # Houston area homeowners have had to handle the recovery from # HurricaneHarvey. @ abscribehttps://twitter.com/abscribe/status/980435743758782464 …' => [ 0.32321514  0.40070974  0.16324385  0.11283126]\n",
      "'Earlier this week, @ RickieFowler joined us to help rebuild the Gilbert family’s home, which was impacted during # HurricaneHarvey. Thank you @ RickieFowler and @ SBPUSA as we work together to rebuild Houston. # FarmersCarespic.twitter.com/mmvzO5GkAR' => [ 0.24082443  0.56621687  0.12624279  0.06671591]\n",
      "'Hurricane Harvey Took our my home church! # hurricaneharvey # mychurch… https://www.instagram.com/p/BhCT4AbhKAM0rzfXbqtQROnFg8SjhxRhLjM5bo0/ …' => [ 0.41836061  0.35541088  0.09402084  0.13220768]\n",
      "'# SisAburto just reminded me of the unity our community experienced during # HurricaneHarvey. My heart will be forever changed by the love every man and woman expressed for each other during that time. We were truly one people to get through that tragedy. # LDSConf' => [ 0.30721356  0.36775172  0.17494696  0.15008776]\n",
      "\"Help for high school students affected by # Harvey # hurricaneharvey. Check out my classroom on @ DonorsChoose! I'd love your help to bring my project to life: https://donorschoo.se/e/eBRAgs1qLL via @ donorschoose\" => [ 0.54463674  0.1687653   0.1442582   0.14233977]\n",
      "'During # HurricaneHarvey, first responders & neighborhoods around an Arkema chemical plant in Crosby say they were exposed to a “toxic cloud” of chemicals. But some argue the blame isn’t completely on Arkema – it’s on lax chemical regulations. http://trib.it/9W pic.twitter.com/Onz1i86AZg' => [ 0.3208386   0.37924358  0.1336091   0.16630872]\n",
      "'Houston # EMS providers learned a lot regarding # hurricane preparation and # emergencyresponse during # HurricaneHarvey and the resulting flooding. Read about their experiences and how they plan to improve things moving forward: http://ow.ly/vqJB30j5tJe' => [ 0.34929736  0.37631272  0.11331557  0.16107435]\n",
      "'I guess he forgave you for your lack of action after the hurricane and then the greed based action when people called you on it # HurricaneHarvey # houston' => [ 0.3493831   0.33787649  0.11047701  0.2022634 ]\n",
      "'Recovery is a work in progress on the Texas coastline # PortAransas # HurricaneHarvey https://www.washingtonpost.com/national/recovery-is-a-work-in-progress-on-the-texas-coastline/2018/03/31/16c8cb44-2d22-11e8-8688-e053ba58f1e4_story.html …' => [ 0.35423621  0.34449049  0.11765273  0.18362058]\n",
      "'One homeowner vowed to not let # HurricaneHarvey run him away. Another is now afraid of the sound driving rain. They both made a decision. Brutal Choice in Houston: Sell Home at a Loss or Face New Floods https://nyti.ms/2uDOmUi @ nytimes @ NYTNational' => [ 0.21589632  0.44329741  0.2360475   0.10475876]\n",
      "'I want to help # HurricaneHarvey' => [ 0.61938605  0.16717379  0.03210667  0.18133348]\n",
      "\"@ AmChemistry's Cal Dooley reviews what the # chemical industry and regulators learned during and after # HurricaneHarvey. http://www.smartbrief.com/original/2018/03/accs-cal-dooley-discusses-what-chemical-industry-and-regulators-have?utm_source=brief …\" => [ 0.37291468  0.2387378   0.19504483  0.19330268]\n",
      "\"Build the # WorldSeries Thx for home an hour on climate in # HurricaneHarvey victims of Prayer for Dr. Liu's Release Terror\" => [ 0.32832018  0.32977233  0.17172066  0.17018684]\n",
      "\"# RT @ zensoinsights: Here Are All the # Senators Who Voted 'No' on # HurricaneHarvey Relief http://bit.ly/2E9kj6n\" => [ 0.50963423  0.0923386   0.19125759  0.20676958]\n",
      "'Lessons from # HurricaneHarvey https://twitter.com/scroll_in/status/980303241832525824 …' => [ 0.37291468  0.2387378   0.19504483  0.19330268]\n",
      "'The \"Texas strong\" collection from Style Of Awesome! Checkout my website for other great tshirts and awesome discounts! https://shop.spreadshirt.com/styleofawesome/ # styleofawesome # love # texasstrong # houston # houstonstrong # hurricaneharvey # models # gym # fitness # muscular # healthy # fitpic.twitter.com/d2zZwRm7Rh' => [ 0.33866911  0.3664815   0.13807218  0.15677721]\n",
      "'U.S. Secretary of Commerce Wilbur Ross determined a commercial # fishery failure in the state of # Texas after # HurricaneHarvey caused a fishery resource disaster. https://www.tradeonlytoday.com/industry-news/commerce-secretary-declares-fishery-disaster-following-hurricane-harvey … # environment # fish' => [ 0.34577375  0.35062117  0.12437108  0.17923401]\n",
      "Total tweets analyzed 131\n",
      "Total tweets classified as DONATION 6 4.580152671755725%\n",
      "Total tweets classified as OTHER 12 9.16030534351145%\n",
      "Total tweets classified as RELOCATION 2 1.5267175572519083%\n",
      "Total tweets classified as VOLUNTEERING 8 6.106870229007633%\n",
      "Total tweets classified as NOT CLASSIFIED 103 78.62595419847328%\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "# Start running time\n",
    "t0 = time()\n",
    "\n",
    "# Get test data from recent tweets\n",
    "harvey_recent_df = pkl.load(open(\"harvey_april_2018.pkl\", \"rb\"))\n",
    "#harvey_tweets = tweet_processor.get_data(path=test_file)\n",
    "#harvey_tweets.columns=['text']\n",
    "# Drop duplicates\n",
    "harvey_recent_df.drop_duplicates('text', inplace=True)\n",
    "\n",
    "# Clean up testing data set\n",
    "clean_test_df = tweet_processor.process_tweets(harvey_recent_df.text)\n",
    "#harvey_tweets.drop_duplicates('text', inplace=True)\n",
    "#clean_test_df = tweet_processor.process_tweets(harvey_tweets['text'])\n",
    "#clean_test_df\n",
    "# Test the classifier\n",
    "#gs_clf.fit(train_df.text, train_df.target)\n",
    "predicted = gs_clf.predict_proba(clean_test_df)\n",
    "\n",
    "# Print running time for training and predicting\n",
    "print('Total running time', time() - t0)\n",
    "\n",
    "#print(metrics.classification_report(y_test, predicted))\n",
    "print(gs_clf.classes_)\n",
    "\n",
    "#print(metrics.classification_report(y_test, predicted))\n",
    "# Print predicted category for each tweet\n",
    "c_donation=c_other=c_relocation=c_volunteering=c_na=0\n",
    "for tweet, category in zip(harvey_recent_df.text, predicted):\n",
    "    print('%r => %s' % (tweet, category))\n",
    "    # Consider only predictions with > 0.5 probability\n",
    "    if category[0] > 0.5:\n",
    "        c_donation += 1\n",
    "    elif category[1] > 0.5:\n",
    "        c_other += 1\n",
    "    elif category[2] > 0.5:\n",
    "        c_relocation += 1\n",
    "    elif category[3] > 0.5:\n",
    "        c_volunteering += 1\n",
    "    else:\n",
    "        c_na += 1\n",
    "\n",
    "total_tweets = len(harvey_recent_df.text)\n",
    "print(\"Total tweets analyzed\", total_tweets)\n",
    "print(\"Total tweets classified as DONATION {} {}%\"\n",
    "      .format(c_donation, (c_donation/total_tweets)*100))\n",
    "print(\"Total tweets classified as OTHER {} {}%\"\n",
    "      .format(c_other, (c_other/total_tweets)*100))\n",
    "print(\"Total tweets classified as RELOCATION {} {}%\"\n",
    "      .format(c_relocation, (c_relocation/total_tweets)*100))\n",
    "print(\"Total tweets classified as VOLUNTEERING {} {}%\"\n",
    "      .format(c_volunteering, (c_volunteering/total_tweets)*100))\n",
    "print(\"Total tweets classified as NOT CLASSIFIED {} {}%\"\n",
    "      .format(c_na, (c_na/total_tweets)*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.68832782,  0.11441899,  0.06118224,  0.13607094],\n",
       "       [ 0.34031426,  0.30528779,  0.1779939 ,  0.17640405],\n",
       "       [ 0.34449734,  0.29674851,  0.18018177,  0.17857238],\n",
       "       ..., \n",
       "       [ 0.37510749,  0.35454523,  0.07590795,  0.19443934],\n",
       "       [ 0.34449734,  0.29674851,  0.18018177,  0.17857238],\n",
       "       [ 0.41851691,  0.24623704,  0.15017721,  0.18506884]])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "donation        116\n",
       "other            75\n",
       "relocation       70\n",
       "volunteering     72\n",
       "dtype: int64"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(['target']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SGDClassifier' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-278-4e1108b75924>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Create list of coefficients and words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcoefs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SGDClassifier' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "coefs = [] \n",
    "words = []\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words=\"english\")\n",
    "tfidf_vectorizer.fit_transform(train_df.text)\n",
    "\n",
    "# Create list of coefficients and words\n",
    "for k,v in tfidf_vectorizer.vocabulary_.items():\n",
    "    coefs.append(clf.named_steps['clf'].coef_[0][tfidf_vectorizer.vocabulary_[k]])\n",
    "    words.append(k)\n",
    "\n",
    "# Create pairs of (words, coefficients)\n",
    "pairs = []\n",
    "for p in zip(words, coefs): \n",
    "    pairs.append(p)\n",
    "\n",
    "# Sort pairs by coefficient\n",
    "pairs.sort(reverse=True, key=operator.itemgetter(1))\n",
    "\n",
    "# Print words with highest coefficients \n",
    "[print(k,v) for k,v in pairs[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
