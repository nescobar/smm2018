{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "consumer_key = '' # Remove after execution\n",
    "consumer_secret = '' # Remove after execution\n",
    "\n",
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_search_tweets(query=\"\", limit=10):\n",
    "    tweets = []\n",
    "\n",
    "    if query == \"\":\n",
    "        print(\"Query not provided\")\n",
    "        return None\n",
    "\n",
    "    cursor = tweepy.Cursor(api.search, q=query, lang=\"en\")\n",
    "    for status in cursor.items(limit):\n",
    "        tweets.append(status.text)\n",
    "    return tweets\n",
    "\n",
    "#hurricane_harvey_1 = get_search_tweets(\"#HurricaneHarvey\", 1000)  # Get 500 tweets from Hurricane Harvey hashtag\n",
    "#hurricane_harvey_2 = get_search_tweets(\"#HarveyRelief\", 500)  # Get 500 tweets from Hurricane Harvey hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import pickle as pkl\n",
    "#import pandas as pd\n",
    "\n",
    "#harvey_1_df = pd.DataFrame({'text':hurricane_harvey_1})\n",
    "#harvey_2_df = pd.DataFrame({'text':hurricane_harvey_2})\n",
    "\n",
    "#harvey_df = pd.concat([harvey_1_df, harvey_2_df])\n",
    "#harvey_df.drop_duplicates('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pkl.dump(harvey_df, open(\"harvey_recent_20180331.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import got3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<got3.manager.TweetCriteria.TweetCriteria at 0x111a0d7b8>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetCriteria = got3.manager.TweetCriteria()\n",
    "tweetCriteria.setQuerySearch(\"HarveyRelief\")\n",
    "tweetCriteria.setSince(\"2018-03-01\")\n",
    "tweetCriteria.setUntil(\"2018-03-31\")\n",
    "tweetCriteria.setMaxTweets(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = got3.manager.TweetManager.getTweets(tweetCriteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_tweets = []\n",
    "for tweet in tweets:\n",
    "    txt_tweets.append(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.DataFrame({'text':txt_tweets})\n",
    "pkl.dump(tweets_df, open(\"harvey_march_relief_2018.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
